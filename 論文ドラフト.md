# SNN-Comprypto: arXiv論文ドラフト

## Title
**SNN-Comprypto: Spiking Neural Network-based Simultaneous Compression and Encryption Using Chaotic Reservoir Dynamics**

---

## Abstract
[TODO: 最後に書く]

---

## 1. Introduction

### 日本語原稿（ユーザー提供）
> 現代の暗号化技術や乱数生成技術では、100％突破されないと言い切れるものはないだろう。
> しかし、もし人間に何かランダムな数字を思い浮かべさせる場合、それはどんな脳波計をつけさせていようが現代の超優秀なAIを使おうが、その数字を当てることは不可能だろう。
> 本研究は人間の脳に近い動作をするスパイキングニューラルネットワーク（以下SNN）を使い、そのランダムな数値を生成する能力を再現することに挑戦したものである。
> 更に、SNNによる情報の暗号化の際には情報量を圧縮し、復号化の際には圧縮された情報を復元できることが確認できたため、人間の情報の要約能力の解明にも繋がっていく可能性も示唆される。

### English Translation (Academic Style)

No existing encryption or random number generation technology can claim absolute invulnerability to attacks. However, when a human is asked to think of a random number, predicting that number remains practically impossible—regardless of whether brain-wave monitors or state-of-the-art artificial intelligence systems are employed. This unpredictability stems from the chaotic dynamics inherent in biological neural circuits.

This study explores the reproduction of this biological randomness using Spiking Neural Networks (SNNs), which closely mimic the temporal dynamics of the human brain. We propose **SNN-Comprypto**, a novel system that leverages the chaotic membrane potential fluctuations of a reservoir computing architecture to generate cryptographically secure keystreams.

Furthermore, our experiments demonstrate that the SNN-based encryption process simultaneously achieves data compression through predictive coding, and the original data can be perfectly reconstructed during decryption. This finding suggests potential implications for understanding the neural mechanisms underlying human information summarization and memory consolidation.

### Key Contributions
- A unified architecture that performs **compression and encryption simultaneously** within a single SNN reservoir
- Exploitation of **short-term synaptic plasticity (MD/LD)** inspired by the hippocampal dentate gyrus for chaotic key generation
- Achievement of **7.5× speedup** over baseline Python implementation using Numba JIT compilation
- Successful passage of all **NIST SP 800-22 randomness tests**

---

## 2. Related Work（関連研究）

### 日本語

SNNを用いた暗号化技術の先行研究は、大きく以下の3つに分類できる。

#### 2.1 生体模倣型暗号（BioEncryptSNN等）
2025年に提案されたBioEncryptSNNは、スパイクのタイミングを利用してデータを暗号化する手法である。報告によれば、AES（PyCryptodome実装）と比較して最大4.1倍の速度向上を達成している。しかし、この手法は主に**符号化スキーム**であり、本研究のような**鍵ストリーム生成器**としてのSNN利用とは異なる。

#### 2.2 カオスニューラルネットワーク（CNN）
多くの「ニューラル暗号」研究は、シグモイド関数やロジスティック写像を用いた連続値ニューロンモデルを採用している。これらは「スパイク」という離散事象を扱わないため、厳密にはSNNとは異なる。また、メモリスタ（記憶抵抗素子）を用いた物理的カオス生成の研究もあるが、ハードウェア依存性が高い。

#### 2.3 Deep Lossless Compression
RNNやLSTMを用いた予測圧縮（DeepZip等）は、次の文字の出現確率を予測し、算術符号化で圧縮する手法である。本研究はこのロジックを踏襲しつつ、計算コストの高いLSTMを**スパイキングリザーバ**に置き換えた点に新規性がある。

#### 2.4 本研究の位置づけ
既存研究が「暗号化のみ」または「圧縮のみ」に焦点を当てる中、本研究は**圧縮と暗号化を単一のSNNダイナミクス内で同時に実行**する点で独自性を持つ。さらに、海馬歯状回のパターン分離機能を模倣したMD/LD可塑性を利用する点は、他の研究には見られない生物学的忠実度である。

---

## 3. Proposed Method（提案手法）

### 日本語

#### 3.1 システム概要：「双子の脳」アーキテクチャ
本システムは、送信者と受信者が**同一の構造・同一の初期状態を持つSNNリザーバ**（以下「脳」）を共有することで動作する。同じシード値で初期化された二つの脳は、同じ入力に対して同じ内部状態遷移を起こすため、**秘密鍵を直接送信することなく**同期可能である。

#### 3.2 ニューロンモデル
本研究ではLeaky Integrate-and-Fire（LIF）ニューロンを採用した。膜電位 V(t) は以下の微分方程式に従う：

```
τ_m × dV/dt = -(V - V_rest) + R × I(t)
```

ここで、τ_m は膜時定数（20ms）、V_rest は静止電位（-65mV）、I(t) はシナプス入力電流である。膜電位が閾値 V_thresh（-50mV）を超えると発火し、V_reset（-70mV）にリセットされる。

#### 3.3 リザーバコンピューティング
300個のLIFニューロンで構成されるリカレントネットワーク（リザーバ）を使用した。結合重みはランダムに初期化され、スペクトル半径を1.4に調整することで「カオスの縁（Edge of Chaos）」に維持される。この領域では、入力の微細な履歴に敏感に反応し続けるカオス的挙動が得られる。

#### 3.4 処理フロー
暗号化・圧縮のフローは以下の通り：

1. **予測（Predict）**: リザーバが次のデータバイトを予測する
2. **残差計算（Compress）**: 実際の値 - 予測値 = 残差（これが圧縮された情報）
3. **鍵生成（Key Generation）**: 全ニューロンの膜電位をSHA-256でハッシュ化し、1バイトの鍵を生成
4. **暗号化（Encrypt）**: 残差 XOR 鍵 = 暗号文
5. **学習（Train）**: 実際の値でリザーバをオンライン学習し、予測精度を向上

復号側では、同じシード値で初期化したリザーバが同じ予測・同じ鍵を生成できるため、暗号文から元のデータを完全に復元できる。

#### 3.5 生物学的インスピレーション
本研究は海馬歯状回の顆粒細胞からインスピレーションを得ている。この細胞は「パターン分離」機能を持ち、類似した入力を互いに重なりのない出力に変換する。これは暗号技術における「雪崩効果（Avalanche Effect）」と生物学的に等価である。

---

## 4. Experiments（実験）

### 日本語原稿（ユーザー提供）
> テストデータとして、次のデータが予測しやすい「正弦波」と、英語の文字列をバイナリ化したもので行った。
> 手法で説明した、圧縮からの暗号化プロセスにより、NISTの乱数テストも全て合格するランダム性の再現に成功した。
> また、試しに暗号化に使った暗号鍵のシード値に1を足した値の復号鍵で復号を試したが、復号データの一致率は0.7%だった。

### 4.1 実験環境
- Python 3.13, Numba JIT
- CPU: Ryzen AI 9 HX 375
- GPU: RTX 5080 Laptop（今回は未使用、将来のCUDA対応用）

### 4.2 テストデータ
| データ | サイズ | 特徴 |
|--------|--------|------|
| 正弦波 | 1000 bytes | 予測しやすい周期的パターン |
| 英語テキスト | 1950 bytes | 自然言語のバイト列 |

### 4.3 整合性テスト
両データセットにおいて、暗号化→復号を行い、**100%の完全復元**を確認した。

| データ | 暗号化時間 | 復号時間 | 整合性 |
|--------|-----------|---------|--------|
| 正弦波 | 565 ms | 210 ms | ✅ 100% |
| テキスト | - | - | ✅ 100% |

### 4.4 NIST SP 800-22 乱数検定
生成された暗号鍵のランダム性を、NIST標準の統計的検定で評価した。

| テスト名 | P値 | 結果 |
|----------|-----|------|
| Frequency (Monobit) | 0.62 | PASS |
| Block Frequency | 0.33 | PASS |
| Runs | 0.25 | PASS |
| Longest Run | 0.84 | PASS |
| Matrix Rank | 0.50 | PASS |
| DFT (Spectral) | 0.13 | PASS |
| Overlapping Template | 0.46 | PASS |
| Approximate Entropy | 0.89 | PASS |
| Cumulative Sums | 0.27 | PASS |
| **合計** | 9/9 | **ALL PASS** |

### 4.5 アバランチ効果
暗号の安全性を確認するため、鍵シード値を1だけ変更した場合の復号結果を検証した。

| 条件 | 復号一致率 |
|------|-----------|
| 正しい鍵（seed=12345） | 100% |
| 誤った鍵（seed=12346） | **0.70%** |

理論的にランダムな場合の期待値は約0.39%（1/256）であり、0.70%という結果はこれに近い。これは、わずか1の鍵の違いで出力が完全に乱数化されることを示しており、高いセキュリティ強度を持つことが確認された。

---

## 5. Conclusion（結論）

### 日本語
本研究では、スパイキングニューラルネットワーク（SNN）のカオス的ダイナミクスを利用した新しい暗号化・圧縮システム「SNN-Comprypto」を提案した。

主な成果：
1. **圧縮と暗号化の同時実行**: 予測符号化とカオス鍵生成を単一のSNNリザーバ内で統合
2. **高速化**: Numba JITによる7.5倍の処理速度向上
3. **暗号論的安全性**: NIST SP 800-22の全9テストに合格
4. **アバランチ効果**: 鍵の微小な変化で出力が完全に乱数化

今後の展望として、GPU (CUDA) 対応による更なる高速化、物理ノイズ（CPU温度等）の入力による真性乱数生成器への発展、およびニューロモルフィックチップ（Intel Loihi等）への実装を検討している。

---

## References

1. Tsodyks, M., & Markram, H. (1998). The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability. *PNAS*.
2. NIST SP 800-22 Rev. 1a: A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications.
3. [BioEncryptSNN] Privacy-Preserving Spiking Neural Networks. *arXiv:2510.19537*.
4. Pathak, J., et al. (2018). Using a reservoir computer to learn chaotic attractors. *arXiv:1802.02844*.
