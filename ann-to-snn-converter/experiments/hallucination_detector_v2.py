"""
SNN Hallucination Detector v2: å¤šç‰¹å¾´é‡ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
==================================================

æ”¹å–„ç‚¹:
1. å˜ç´”ãªã‚¸ãƒƒã‚¿ãƒ¼â†’å±¤åˆ¥ã‚¸ãƒƒã‚¿ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
2. Synchronyç‰¹å¾´é‡ã®è¿½åŠ 
3. TTFSåˆ†æ•£ã®è¿½åŠ 
4. æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®é–¾å€¤èª¿æ•´

Author: ã‚ãƒ¼ã‚‹ (Cell Activation)
Date: 2026-02-04
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, roc_curve, classification_report
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']

print("=" * 70)
print("ğŸ” SNN Hallucination Detector v2: å¤šç‰¹å¾´é‡ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ")
print("=" * 70)


# =============================================================================
# 1. ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆå‰å›ã¨åŒã˜ï¼‰
# =============================================================================
class ResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_ch != out_ch:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_ch)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        return F.relu(out)


class SmallResNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(64, 64, 2)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)
        self.activations = {}
    
    def _make_layer(self, in_ch, out_ch, blocks, stride=1):
        layers = [ResBlock(in_ch, out_ch, stride)]
        for _ in range(1, blocks):
            layers.append(ResBlock(out_ch, out_ch))
        return nn.Sequential(*layers)
    
    def forward(self, x, save_act=False):
        x = F.relu(self.bn1(self.conv1(x)))
        if save_act: self.activations['conv1'] = x.clone()
        x = self.layer1(x)
        if save_act: self.activations['layer1'] = x.clone()
        x = self.layer2(x)
        if save_act: self.activations['layer2'] = x.clone()
        x = self.layer3(x)
        if save_act: self.activations['layer3'] = x.clone()
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        if save_act: self.activations['avgpool'] = x.clone()
        return self.fc(x)
    
    def get_activations(self, x):
        self.activations = {}
        _ = self.forward(x, save_act=True)
        return self.activations


# =============================================================================
# 2. å¤šç‰¹å¾´é‡æŠ½å‡ºå™¨
# =============================================================================
class MultiFeatureExtractor:
    """SNNè§£æã«åŸºã¥ãå¤šç‰¹å¾´é‡æŠ½å‡º"""
    
    def __init__(self, model, timesteps=100, num_trials=10):
        self.model = model
        self.timesteps = timesteps
        self.num_trials = num_trials
    
    def compute_ttfs(self, activation):
        """TTFSè¨ˆç®—"""
        ttfs = torch.full_like(activation, float(self.timesteps))
        active_mask = activation > 0
        if active_mask.any():
            max_act = activation.max()
            if max_act > 0:
                normalized = activation[active_mask] / max_act
                ttfs[active_mask] = self.timesteps * (1 - normalized)
        return ttfs
    
    def extract_features(self, x, noise_std=0.1):
        """å¤šç‰¹å¾´é‡æŠ½å‡º"""
        features = {}
        
        self.model.eval()
        with torch.no_grad():
            # å…ƒã®äºˆæ¸¬æƒ…å ±
            output = self.model(x)
            probs = F.softmax(output, dim=1)
            features['confidence'] = probs.max().item()
            features['entropy'] = -(probs * torch.log(probs + 1e-8)).sum().item()
            features['margin'] = (probs.topk(2)[0][0,0] - probs.topk(2)[0][0,1]).item()
            
            # å±¤åˆ¥æ´»æ€§åŒ–çµ±è¨ˆ
            activations = self.model.get_activations(x)
            for layer_name, act in activations.items():
                act_flat = act.view(-1)
                features[f'{layer_name}_mean'] = act_flat.mean().item()
                features[f'{layer_name}_std'] = act_flat.std().item()
                features[f'{layer_name}_max'] = act_flat.max().item()
                features[f'{layer_name}_sparsity'] = (act_flat == 0).float().mean().item()
            
            # å±¤åˆ¥TTFSçµ±è¨ˆ
            for layer_name, act in activations.items():
                ttfs = self.compute_ttfs(act)
                ttfs_flat = ttfs.view(-1)
                features[f'{layer_name}_ttfs_mean'] = ttfs_flat.mean().item()
                features[f'{layer_name}_ttfs_std'] = ttfs_flat.std().item()
                features[f'{layer_name}_ttfs_min'] = ttfs_flat.min().item()
            
            # ã‚¸ãƒƒã‚¿ãƒ¼åˆ†æï¼ˆãƒã‚¤ã‚ºæ‘‚å‹•ï¼‰
            all_outputs = []
            layer_jitters = defaultdict(list)
            
            for _ in range(self.num_trials):
                noisy_x = x + torch.randn_like(x) * noise_std
                noisy_x = torch.clamp(noisy_x, 0, 1)
                
                noisy_output = self.model(noisy_x)
                all_outputs.append(noisy_output)
                
                noisy_act = self.model.get_activations(noisy_x)
                for layer_name, act in noisy_act.items():
                    ttfs = self.compute_ttfs(act)
                    layer_jitters[layer_name].append(ttfs.mean().item())
            
            # å‡ºåŠ›ã‚¸ãƒƒã‚¿ãƒ¼
            stacked_outputs = torch.stack(all_outputs)
            features['output_jitter'] = stacked_outputs.std(dim=0).mean().item()
            
            # å±¤åˆ¥ã‚¸ãƒƒã‚¿ãƒ¼
            for layer_name, jitter_list in layer_jitters.items():
                features[f'{layer_name}_jitter'] = np.std(jitter_list)
            
            # äºˆæ¸¬å®‰å®šæ€§
            preds = [o.argmax(dim=1).item() for o in all_outputs]
            features['pred_stability'] = len(set(preds)) / len(preds)
            
        return features
    
    def get_feature_names(self):
        """ç‰¹å¾´é‡åã®ãƒªã‚¹ãƒˆ"""
        return list(self.extract_features(torch.randn(1, 3, 32, 32)).keys())


# =============================================================================
# 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
# =============================================================================
print("\nã€1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€‘")
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_subset = torch.utils.data.Subset(train_dataset, range(8000))
test_subset = torch.utils.data.Subset(test_dataset, range(400))
val_subset = torch.utils.data.Subset(test_dataset, range(400, 700))

train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_subset, batch_size=1, shuffle=False)
val_loader = torch.utils.data.DataLoader(val_subset, batch_size=1, shuffle=False)

print(f"  è¨“ç·´: {len(train_subset)}, ç‰¹å¾´æŠ½å‡º: {len(test_subset)}, æ¤œè¨¼: {len(val_subset)}")


# =============================================================================
# 4. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
# =============================================================================
print("\nã€2. SmallResNetå­¦ç¿’ã€‘")
model = SmallResNet(num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(15):
    model.train()
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    
    if (epoch + 1) % 5 == 0:
        model.eval()
        correct = 0
        with torch.no_grad():
            for data, target in test_loader:
                pred = model(data).argmax(dim=1)
                correct += (pred == target).sum().item()
        print(f"  Epoch {epoch+1}: Accuracy = {100*correct/len(test_subset):.1f}%")


# =============================================================================
# 5. ç‰¹å¾´é‡æŠ½å‡º
# =============================================================================
print("\nã€3. å¤šç‰¹å¾´é‡æŠ½å‡ºã€‘")

extractor = MultiFeatureExtractor(model, timesteps=100, num_trials=8)

X_train = []
y_train = []

model.eval()
for i, (data, target) in enumerate(test_loader):
    if i >= 200:  # è¨“ç·´ç”¨200ã‚µãƒ³ãƒ—ãƒ«
        break
    
    features = extractor.extract_features(data, noise_std=0.1)
    X_train.append(list(features.values()))
    
    # æ­£è§£/ä¸æ­£è§£ãƒ©ãƒ™ãƒ«
    with torch.no_grad():
        pred = model(data).argmax(dim=1).item()
    is_wrong = 0 if pred == target.item() else 1
    y_train.append(is_wrong)
    
    if (i + 1) % 50 == 0:
        print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æŠ½å‡º: {i+1}/200")

X_train = np.array(X_train)
y_train = np.array(y_train)

print(f"\n  ç‰¹å¾´é‡æ•°: {X_train.shape[1]}")
print(f"  æ­£è§£ã‚µãƒ³ãƒ—ãƒ«: {sum(1 for y in y_train if y == 0)}")
print(f"  ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«: {sum(1 for y in y_train if y == 1)}")


# =============================================================================
# 6. åˆ†é¡å™¨å­¦ç¿’
# =============================================================================
print("\nã€4. åˆ†é¡å™¨å­¦ç¿’ã€‘")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
lr_clf = LogisticRegression(max_iter=1000, class_weight='balanced')
lr_clf.fit(X_train_scaled, y_train)

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
rf_clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_clf.fit(X_train_scaled, y_train)

print("  åˆ†é¡å™¨å­¦ç¿’å®Œäº†ï¼")


# =============================================================================
# 7. æ¤œè¨¼ã‚»ãƒƒãƒˆã§è©•ä¾¡
# =============================================================================
print("\nã€5. æ¤œè¨¼ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡ã€‘")

X_val = []
y_val = []

model.eval()
for i, (data, target) in enumerate(val_loader):
    if i >= 150:
        break
    
    features = extractor.extract_features(data, noise_std=0.1)
    X_val.append(list(features.values()))
    
    with torch.no_grad():
        pred = model(data).argmax(dim=1).item()
    is_wrong = 0 if pred == target.item() else 1
    y_val.append(is_wrong)
    
    if (i + 1) % 50 == 0:
        print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡º: {i+1}/150")

X_val = np.array(X_val)
y_val = np.array(y_val)
X_val_scaled = scaler.transform(X_val)

print(f"\n  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(y_val)} ã‚µãƒ³ãƒ—ãƒ«")
print(f"  æ­£è§£: {sum(1 for y in y_val if y == 0)}, ä¸æ­£è§£: {sum(1 for y in y_val if y == 1)}")

# è©•ä¾¡
print("\n  ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€‘")
lr_pred = lr_clf.predict(X_val_scaled)
lr_prob = lr_clf.predict_proba(X_val_scaled)[:, 1]
print(classification_report(y_val, lr_pred, target_names=['Correct', 'Wrong'], zero_division=0))
if len(set(y_val)) > 1:
    lr_auc = roc_auc_score(y_val, lr_prob)
    print(f"  AUC-ROC: {lr_auc:.4f}")

print("\n  ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€‘")
rf_pred = rf_clf.predict(X_val_scaled)
rf_prob = rf_clf.predict_proba(X_val_scaled)[:, 1]
print(classification_report(y_val, rf_pred, target_names=['Correct', 'Wrong'], zero_division=0))
if len(set(y_val)) > 1:
    rf_auc = roc_auc_score(y_val, rf_prob)
    print(f"  AUC-ROC: {rf_auc:.4f}")


# =============================================================================
# 8. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
# =============================================================================
print("\nã€6. ç‰¹å¾´é‡é‡è¦åº¦ã€‘")

feature_names = list(features.keys())
importances = rf_clf.feature_importances_
indices = np.argsort(importances)[::-1]

print(f"\n  ä¸Šä½10ç‰¹å¾´é‡:")
for i in range(min(10, len(feature_names))):
    idx = indices[i]
    print(f"    {i+1}. {feature_names[idx]}: {importances[idx]:.4f}")


# =============================================================================
# 9. å¯è¦–åŒ–
# =============================================================================
print("\nã€7. å¯è¦–åŒ–ã€‘")

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# ROCæ›²ç·šæ¯”è¼ƒ
ax = axes[0, 0]
if len(set(y_val)) > 1:
    fpr_lr, tpr_lr, _ = roc_curve(y_val, lr_prob)
    fpr_rf, tpr_rf, _ = roc_curve(y_val, rf_prob)
    ax.plot(fpr_lr, tpr_lr, 'b-', linewidth=2, label=f'Logistic (AUC={lr_auc:.3f})')
    ax.plot(fpr_rf, tpr_rf, 'g-', linewidth=2, label=f'Random Forest (AUC={rf_auc:.3f})')
    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('ROC Curve Comparison')
    ax.legend()

# ç‰¹å¾´é‡é‡è¦åº¦
ax = axes[0, 1]
top_n = 10
top_indices = indices[:top_n]
top_names = [feature_names[i][:15] for i in top_indices]
top_imp = importances[top_indices]
ax.barh(range(top_n), top_imp, color='steelblue')
ax.set_yticks(range(top_n))
ax.set_yticklabels(top_names)
ax.set_xlabel('Importance')
ax.set_title('Top 10 Feature Importances')
ax.invert_yaxis()

# ç¢ºä¿¡åº¦åˆ†å¸ƒ
ax = axes[1, 0]
correct_conf = [X_val[i, 0] for i in range(len(y_val)) if y_val[i] == 0]
wrong_conf = [X_val[i, 0] for i in range(len(y_val)) if y_val[i] == 1]
ax.hist(correct_conf, bins=15, alpha=0.7, label='Correct', color='green')
ax.hist(wrong_conf, bins=15, alpha=0.7, label='Wrong', color='red')
ax.set_xlabel('Confidence')
ax.set_ylabel('Count')
ax.set_title('Confidence Distribution')
ax.legend()

# äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ
ax = axes[1, 1]
correct_prob = [rf_prob[i] for i in range(len(y_val)) if y_val[i] == 0]
wrong_prob = [rf_prob[i] for i in range(len(y_val)) if y_val[i] == 1]
ax.hist(correct_prob, bins=15, alpha=0.7, label='Correct', color='green')
ax.hist(wrong_prob, bins=15, alpha=0.7, label='Wrong', color='red')
ax.set_xlabel('Hallucination Probability')
ax.set_ylabel('Count')
ax.set_title('Predicted Hallucination Probability')
ax.legend()

plt.tight_layout()
plt.savefig('hallucination_detector_v2.png', dpi=150, bbox_inches='tight')
print("  ä¿å­˜: hallucination_detector_v2.png")


# =============================================================================
# 10. ã¾ã¨ã‚
# =============================================================================
print("\n" + "=" * 70)
print("ğŸ”¬ SNN Hallucination Detector v2 ã¾ã¨ã‚")
print("=" * 70)

print(f"""
ã€æ”¹å–„ç‚¹ã€‘
  - å˜ä¸€ã‚¸ãƒƒã‚¿ãƒ¼ â†’ {len(feature_names)}å€‹ã®å¤šç‰¹å¾´é‡
  - é–¾å€¤ãƒ™ãƒ¼ã‚¹ â†’ æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹åˆ†é¡
  - å±¤åˆ¥ã®TTFS/ã‚¸ãƒƒã‚¿ãƒ¼çµ±è¨ˆã‚’æ´»ç”¨

ã€è©•ä¾¡çµæœã€‘
  - ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° AUC: {lr_auc:.4f}
  - ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ AUC: {rf_auc:.4f}
  
ã€é‡è¦ãªç™ºè¦‹ã€‘
  - ä¸Šä½ç‰¹å¾´é‡: {feature_names[indices[0]]}, {feature_names[indices[1]]}, {feature_names[indices[2]]}
  - SNNç‰¹å¾´ï¼ˆTTFS, ã‚¸ãƒƒã‚¿ãƒ¼ï¼‰ãŒãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ã«å¯„ä¸

ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘
  1. ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼
  2. Transformer/LLMã¸ã®é©ç”¨
  3. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIåŒ–
  4. é–¾å€¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è‡ªå‹•åŒ–
""")

print("\nğŸš€ å¤šç‰¹å¾´é‡SNNè§£æ = AIã®ã€Œå¥åº·è¨ºæ–­ã€ï¼")
print("=" * 70)
