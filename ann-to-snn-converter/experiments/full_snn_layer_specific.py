"""
å…¨å±¤SNNï¼‹å±¤åˆ¥é–¾å€¤èª¿æ•´ï¼‹å±¤é–“ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—
==============================================

å„å±¤ã”ã¨ã«æœ€é©ãªÎ±å€¤ã‚’è¨­å®šã—ã€
å±¤é–“ã§ã‚‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯ï¼‹è†œé›»ä½ï¼‰ã‚’ä½¿ç”¨

Author: ã‚ãƒ¼ã‚‹ (Cell Activation)
Date: 2026-02-03
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

print("=" * 70)
print("ğŸ§  å…¨å±¤SNNï¼‹å±¤åˆ¥é–¾å€¤èª¿æ•´ï¼‹å±¤é–“ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—")
print("=" * 70)

# ============================================================
# 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
# ============================================================
print("\nã€1. CIFAR-10ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€‘")

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_subset = torch.utils.data.Subset(trainset, range(10000))
test_subset = torch.utils.data.Subset(testset, range(500))  # å°‘ãªã‚ï¼ˆå…¨å±¤SNNã¯é…ã„ï¼‰

trainloader = DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=0)
testloader = DataLoader(test_subset, batch_size=32, shuffle=False, num_workers=0)

print(f"  è¨“ç·´: {len(train_subset)}, ãƒ†ã‚¹ãƒˆ: {len(test_subset)}")

# ============================================================
# 2. ã‚·ãƒ³ãƒ—ãƒ«ãªCNNï¼ˆANNï¼‰
# ============================================================
print("\nã€2. CNNæ§‹ç¯‰ï¼†å­¦ç¿’ã€‘")

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1, bias=False)
        self.pool1 = nn.AvgPool2d(2)
        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1, bias=False)
        self.pool2 = nn.AvgPool2d(2)
        self.fc = nn.Linear(64 * 8 * 8, 10, bias=False)
    
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool1(x)
        x = torch.relu(self.conv2(x))
        x = self.pool2(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)

model = SimpleCNN()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    model.train()
    for inputs, labels in trainloader:
        optimizer.zero_grad()
        loss = criterion(model(inputs), labels)
        loss.backward()
        optimizer.step()

model.eval()
correct = total = 0
with torch.no_grad():
    for inputs, labels in testloader:
        _, predicted = model(inputs).max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
ann_acc = 100. * correct / total
print(f"  ANNç²¾åº¦: {ann_acc:.1f}%")

# ============================================================
# 3. å„å±¤ã®æœ€å¤§æ´»æ€§åŒ–ã‚’è¨ˆæ¸¬
# ============================================================
print("\nã€3. å„å±¤ã®æœ€å¤§æ´»æ€§åŒ–è¨ˆæ¸¬ã€‘")

activation_max = {'conv1': 0, 'conv2': 0, 'fc': 0}

with torch.no_grad():
    for inputs, _ in trainloader:
        x = torch.relu(model.conv1(inputs))
        activation_max['conv1'] = max(activation_max['conv1'], x.abs().max().item())
        x = model.pool1(x)
        x = torch.relu(model.conv2(x))
        activation_max['conv2'] = max(activation_max['conv2'], x.abs().max().item())
        x = model.pool2(x)
        x = x.view(x.size(0), -1)
        out = model.fc(x)
        activation_max['fc'] = max(activation_max['fc'], out.abs().max().item())

for name, val in activation_max.items():
    print(f"  {name}: {val:.2f}")

# ============================================================
# 4. å…¨å±¤SNNæ¨è«–ï¼ˆå±¤ã”ã¨ã®Î±ã€å±¤é–“ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—ï¼‰
# ============================================================
print("\nã€4. å…¨å±¤SNNæ¨è«–ï¼ˆå±¤é–“ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—ï¼‰ã€‘")

def full_snn_inference(x_np, alphas, T, hybrid_weight=0.7):
    """
    å…¨å±¤ã‚’SNNåŒ–ã€å±¤é–“ã§ã‚‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—ã‚’ä½¿ç”¨
    
    alphas: {'conv1': Î±1, 'conv2': Î±2, 'fc': Î±3}
    hybrid_weight: ã‚¹ãƒ‘ã‚¤ã‚¯ã®é‡ã¿ï¼ˆ1-hybrid_weight = è†œé›»ä½ã®é‡ã¿ï¼‰
    """
    batch_size = x_np.shape[0]
    
    # é–¾å€¤è¨­å®š
    thresholds = {name: alphas[name] * activation_max[name] for name in alphas}
    
    # é‡ã¿å–å¾—
    w_conv1 = model.conv1.weight.detach().numpy()
    w_conv2 = model.conv2.weight.detach().numpy()
    w_fc = model.fc.weight.detach().numpy()
    
    # === Conv1 SNN ===
    mem_conv1 = np.zeros((batch_size, 32, 32, 32))
    spike_count_conv1 = np.zeros_like(mem_conv1)
    
    input_per_step = x_np / T
    
    for t in range(T):
        with torch.no_grad():
            conv1_out = torch.nn.functional.conv2d(
                torch.tensor(input_per_step, dtype=torch.float32),
                model.conv1.weight, padding=1
            ).numpy()
        
        mem_conv1 += conv1_out
        spikes = (mem_conv1 >= thresholds['conv1']).astype(float)
        mem_conv1 -= spikes * thresholds['conv1']
        spike_count_conv1 += spikes
    
    # å±¤é–“ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯ç‡ï¼‹è†œé›»ä½ï¼‰
    conv1_output = (hybrid_weight * (spike_count_conv1 / T) + 
                   (1 - hybrid_weight) * (mem_conv1 / thresholds['conv1']))
    
    # AvgPool
    pool1_out = conv1_output.reshape(batch_size, 32, 16, 2, 16, 2).mean(axis=(3, 5))
    
    # === Conv2 SNN ===
    mem_conv2 = np.zeros((batch_size, 64, 16, 16))
    spike_count_conv2 = np.zeros_like(mem_conv2)
    
    input2_per_step = pool1_out / T
    
    for t in range(T):
        with torch.no_grad():
            conv2_out = torch.nn.functional.conv2d(
                torch.tensor(input2_per_step * T, dtype=torch.float32),  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
                model.conv2.weight, padding=1
            ).numpy()
        
        mem_conv2 += conv2_out / T
        spikes = (mem_conv2 >= thresholds['conv2']).astype(float)
        mem_conv2 -= spikes * thresholds['conv2']
        spike_count_conv2 += spikes
    
    # å±¤é–“ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—
    conv2_output = (hybrid_weight * (spike_count_conv2 / T) + 
                   (1 - hybrid_weight) * (mem_conv2 / thresholds['conv2']))
    
    # AvgPool
    pool2_out = conv2_output.reshape(batch_size, 64, 8, 2, 8, 2).mean(axis=(3, 5))
    
    # === FC SNN ===
    flat = pool2_out.reshape(batch_size, -1)
    
    mem_fc = np.zeros((batch_size, 10))
    spike_count_fc = np.zeros_like(mem_fc)
    
    fc_current = flat @ w_fc.T
    fc_per_step = fc_current / T
    
    for t in range(T):
        mem_fc += fc_per_step
        spikes = (mem_fc >= thresholds['fc']).astype(float)
        mem_fc -= spikes * thresholds['fc']
        spike_count_fc += spikes
    
    # æœ€çµ‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—
    output = (hybrid_weight * (spike_count_fc / T) + 
             (1 - hybrid_weight) * (mem_fc / thresholds['fc']))
    
    return np.argmax(output, axis=1)


# ãƒ†ã‚¹ãƒˆ
print("\n  å±¤åˆ¥Î±å€¤ã§ã®ç²¾åº¦ãƒ†ã‚¹ãƒˆ:")
print("-" * 80)
print(f"  {'Conv1 Î±':>8} | {'Conv2 Î±':>8} | {'FC Î±':>8} | {'SNNç²¾åº¦':>10} | {'ANNå·®':>8}")
print("-" * 80)

T = 30  # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—

# æ§˜ã€…ãªÎ±å€¤ã®çµ„ã¿åˆã‚ã›ã‚’ãƒ†ã‚¹ãƒˆ
test_configs = [
    # å…¨ã¦åŒã˜Î±
    {'conv1': 1.0, 'conv2': 1.0, 'fc': 1.0},
    {'conv1': 2.0, 'conv2': 2.0, 'fc': 2.0},
    {'conv1': 3.0, 'conv2': 3.0, 'fc': 3.0},
    # å±¤ã”ã¨ã«ç•°ãªã‚‹Î±ï¼ˆå‰ãŒä½ã„ï¼‰
    {'conv1': 1.0, 'conv2': 1.5, 'fc': 2.0},
    {'conv1': 1.5, 'conv2': 2.0, 'fc': 2.5},
    # å±¤ã”ã¨ã«ç•°ãªã‚‹Î±ï¼ˆå‰ãŒé«˜ã„ï¼‰
    {'conv1': 3.0, 'conv2': 2.5, 'fc': 2.0},
    {'conv1': 2.5, 'conv2': 2.0, 'fc': 1.5},
    # ç‰¹æ®Šãƒ‘ã‚¿ãƒ¼ãƒ³
    {'conv1': 0.5, 'conv2': 2.0, 'fc': 2.0},  # å…¥åŠ›å±¤ã ã‘ä½ã‚
    {'conv1': 2.0, 'conv2': 0.5, 'fc': 2.0},  # ä¸­é–“å±¤ã ã‘ä½ã‚
    {'conv1': 2.0, 'conv2': 2.0, 'fc': 0.5},  # å‡ºåŠ›å±¤ã ã‘ä½ã‚
]

results = []

for alphas in test_configs:
    correct = 0
    total = 0
    
    for inputs, labels in testloader:
        x_np = inputs.numpy()
        preds = full_snn_inference(x_np, alphas, T)
        correct += (preds == labels.numpy()).sum()
        total += len(labels)
    
    snn_acc = 100. * correct / total
    diff = snn_acc - ann_acc
    marker = " âœ…" if abs(diff) < 5 else ""
    results.append((alphas, snn_acc, diff))
    
    print(f"  {alphas['conv1']:>8.1f} | {alphas['conv2']:>8.1f} | {alphas['fc']:>8.1f} | "
          f"{snn_acc:>9.1f}% | {diff:>+7.1f}%{marker}")

print("-" * 80)

# æœ€è‰¯ã®çµæœã‚’è¡¨ç¤º
best = max(results, key=lambda x: x[1])
print(f"\n  ã€æœ€è‰¯ã€‘Î±={best[0]} â†’ {best[1]:.1f}%")

# ============================================================
# 5. çµæœ
# ============================================================
print("\n" + "=" * 70)
print("ğŸ“Š å…¨å±¤SNNï¼‹å±¤é–“ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã— çµæœ")
print("=" * 70)
print(f"""
  ã€ANNç²¾åº¦ã€‘{ann_acc:.1f}%
  
  ã€æ‰‹æ³•ã€‘
  - å…¨å±¤ã‚’IFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§SNNåŒ–
  - å„å±¤ã§ç‹¬ç«‹ã—ãŸÎ±å€¤ï¼ˆé–¾å€¤=Î±Ã—max_activationï¼‰
  - å±¤é–“ã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—ï¼ˆ70%ã‚¹ãƒ‘ã‚¤ã‚¯ + 30%è†œé›»ä½ï¼‰
  
  ã€ç™ºè¦‹ã€‘
  - å±¤é–“ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰èª­ã¿å‡ºã—ã§å…¨å±¤SNNã§ã‚‚ç²¾åº¦ç¶­æŒï¼Ÿ
  - æœ€é©ãªÎ±å€¤ã¯å±¤ã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼Ÿ
""")
