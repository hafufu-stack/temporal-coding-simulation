"""
Transformer SNN Analysis: Attentionå±¤ã®SNNè§£æ
==============================================

Transformerãƒ¢ãƒ‡ãƒ«ï¼ˆBERT/ViTé¢¨ï¼‰ã®ä¸­é–“å±¤ã‚’SNNç‰¹å¾´é‡ã§è§£æã—ã€
ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ã«é©ç”¨ã™ã‚‹ã€‚

Key Insight:
- Attentionå±¤ã®æ´»æ€§åŒ– â†’ TTFSå¤‰æ› â†’ æ€è€ƒå„ªå…ˆåº¦
- LayerNormå¾Œã®å€¤ â†’ ã‚¸ãƒƒã‚¿ãƒ¼è§£æ â†’ å®‰å®šæ€§

Author: ã‚ãƒ¼ã‚‹ (Cell Activation)
Date: 2026-02-04
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import math
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']

print("=" * 70)
print("ğŸ§  Transformer SNN Analysis: Attentionå±¤ã®ã‚¹ãƒ‘ã‚¤ã‚¯è§£æ")
print("=" * 70)


# =============================================================================
# 1. Mini Transformer (ViTé¢¨)
# =============================================================================
class MultiHeadAttention(nn.Module):
    """Multi-Head Self-Attention"""
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        self.attention_weights = None  # ä¿å­˜ç”¨
    
    def forward(self, x):
        B, N, C = x.shape
        
        q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        
        attn = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn = F.softmax(attn, dim=-1)
        self.attention_weights = attn.detach()  # ä¿å­˜
        
        out = (attn @ v).transpose(1, 2).reshape(B, N, C)
        return self.out_proj(out)


class TransformerBlock(nn.Module):
    """Transformer Encoder Block"""
    def __init__(self, d_model, num_heads, mlp_ratio=4.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.norm2 = nn.LayerNorm(d_model)
        self.mlp = nn.Sequential(
            nn.Linear(d_model, int(d_model * mlp_ratio)),
            nn.GELU(),
            nn.Linear(int(d_model * mlp_ratio), d_model)
        )
        
        # ä¸­é–“æ´»æ€§åŒ–ä¿å­˜
        self.post_attn = None
        self.post_mlp = None
    
    def forward(self, x):
        # Attention
        attn_out = self.attn(self.norm1(x))
        x = x + attn_out
        self.post_attn = x.detach()
        
        # MLP
        mlp_out = self.mlp(self.norm2(x))
        x = x + mlp_out
        self.post_mlp = x.detach()
        
        return x


class MiniViT(nn.Module):
    """Mini Vision Transformer for CIFAR-10"""
    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10,
                 d_model=128, depth=4, num_heads=4):
        super().__init__()
        self.patch_size = patch_size
        num_patches = (img_size // patch_size) ** 2
        
        # Patch Embedding
        self.patch_embed = nn.Conv2d(in_chans, d_model, kernel_size=patch_size, stride=patch_size)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, d_model))
        
        # Transformer Blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads) for _ in range(depth)
        ])
        
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, num_classes)
        
        nn.init.normal_(self.pos_embed, std=0.02)
        nn.init.normal_(self.cls_token, std=0.02)
    
    def forward(self, x):
        B = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x)  # (B, d_model, H/P, W/P)
        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, d_model)
        
        # Add cls token and position embedding
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        x = x + self.pos_embed
        
        # Transformer blocks
        for block in self.blocks:
            x = block(x)
        
        x = self.norm(x)
        cls_out = x[:, 0]
        return self.head(cls_out)
    
    def get_all_activations(self):
        """å…¨ãƒ–ãƒ­ãƒƒã‚¯ã®æ´»æ€§åŒ–ã‚’å–å¾—"""
        activations = {}
        for i, block in enumerate(self.blocks):
            activations[f'block{i}_attn'] = block.post_attn
            activations[f'block{i}_mlp'] = block.post_mlp
            activations[f'block{i}_attention_weights'] = block.attn.attention_weights
        return activations


# =============================================================================
# 2. Transformerå‘ã‘SNNè§£æ
# =============================================================================
class TransformerSNNAnalyzer:
    """Transformerå±¤ã®SNNè§£æ"""
    
    def __init__(self, timesteps=100, num_trials=10):
        self.timesteps = timesteps
        self.num_trials = num_trials
    
    def compute_ttfs(self, activation):
        """TTFSè¨ˆç®—"""
        ttfs = torch.full_like(activation, float(self.timesteps))
        active_mask = activation > 0
        if active_mask.any():
            max_act = activation.max()
            if max_act > 0:
                normalized = activation[active_mask] / max_act
                ttfs[active_mask] = self.timesteps * (1 - normalized)
        return ttfs
    
    def analyze_attention_ttfs(self, attention_weights):
        """Attentioné‡ã¿ã®TTFSè§£æ
        
        é«˜ã„Attention = æ—©ã„ã‚¹ãƒ‘ã‚¤ã‚¯ = ãã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å„ªå…ˆçš„ã«è¦‹ã¦ã„ã‚‹
        """
        # attention_weights: (B, num_heads, N, N)
        # å¹³å‡åŒ–: å„ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®ç·æ³¨æ„åº¦
        avg_attn = attention_weights.mean(dim=1)  # (B, N, N)
        incoming_attn = avg_attn.mean(dim=1)  # (B, N) - å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒå—ã‘ã‚‹æ³¨æ„
        outgoing_attn = avg_attn.mean(dim=2)  # (B, N) - å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸ãˆã‚‹æ³¨æ„
        
        ttfs_incoming = self.compute_ttfs(incoming_attn)
        ttfs_outgoing = self.compute_ttfs(outgoing_attn)
        
        return {
            'ttfs_incoming': ttfs_incoming,  # ã©ã‚Œã ã‘æ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã‹
            'ttfs_outgoing': ttfs_outgoing,  # ã©ã‚Œã ã‘ä»–ã‚’æ³¨ç›®ã—ã¦ã„ã‚‹ã‹
            'mean_incoming': ttfs_incoming.mean().item(),
            'mean_outgoing': ttfs_outgoing.mean().item()
        }
    
    def compute_attention_synchrony(self, attention_weights, tolerance=0.1):
        """AttentionåŒæœŸåº¦: è¤‡æ•°ãƒ˜ãƒƒãƒ‰ãŒåŒã˜å ´æ‰€ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹"""
        # attention_weights: (B, num_heads, N, N)
        num_heads = attention_weights.size(1)
        
        # å„ãƒ˜ãƒƒãƒ‰ã®max attentionä½ç½®
        max_attn_per_head = attention_weights.argmax(dim=-1)  # (B, num_heads, N)
        
        # ãƒ˜ãƒƒãƒ‰é–“ã®ä¸€è‡´åº¦
        sync_score = 0
        count = 0
        for i in range(num_heads):
            for j in range(i+1, num_heads):
                agreement = (max_attn_per_head[:, i] == max_attn_per_head[:, j]).float().mean()
                sync_score += agreement.item()
                count += 1
        
        return sync_score / count if count > 0 else 0
    
    def compute_stability(self, model, x, noise_std=0.05):
        """Transformeræ´»æ€§åŒ–ã®å®‰å®šæ€§"""
        all_activations = defaultdict(list)
        
        model.eval()
        with torch.no_grad():
            for _ in range(self.num_trials):
                noisy_x = x + torch.randn_like(x) * noise_std
                noisy_x = torch.clamp(noisy_x, 0, 1)
                
                _ = model(noisy_x)
                activations = model.get_all_activations()
                
                for name, act in activations.items():
                    if act is not None:
                        all_activations[name].append(act)
        
        # ã‚¸ãƒƒã‚¿ãƒ¼è¨ˆç®—
        stability = {}
        for name, act_list in all_activations.items():
            stacked = torch.stack(act_list)
            jitter = stacked.std(dim=0).mean().item()
            stability[name] = jitter
        
        return stability
    
    def extract_features(self, model, x, noise_std=0.05):
        """çµ±åˆç‰¹å¾´é‡æŠ½å‡º"""
        features = {}
        
        model.eval()
        with torch.no_grad():
            output = model(x)
            probs = F.softmax(output, dim=1)
            
            features['confidence'] = probs.max().item()
            features['entropy'] = -(probs * torch.log(probs + 1e-8)).sum().item()
            features['margin'] = (probs.topk(2)[0][0,0] - probs.topk(2)[0][0,1]).item()
            
            # Attentionè§£æ
            activations = model.get_all_activations()
            
            for i, block in enumerate(model.blocks):
                attn_weights = block.attn.attention_weights
                if attn_weights is not None:
                    attn_analysis = self.analyze_attention_ttfs(attn_weights)
                    features[f'block{i}_ttfs_incoming'] = attn_analysis['mean_incoming']
                    features[f'block{i}_ttfs_outgoing'] = attn_analysis['mean_outgoing']
                    
                    sync = self.compute_attention_synchrony(attn_weights)
                    features[f'block{i}_head_sync'] = sync
                
                # MLPæ´»æ€§åŒ–çµ±è¨ˆ
                if block.post_mlp is not None:
                    act = block.post_mlp
                    features[f'block{i}_mlp_mean'] = act.mean().item()
                    features[f'block{i}_mlp_std'] = act.std().item()
            
            # å®‰å®šæ€§
            stability = self.compute_stability(model, x, noise_std)
            for name, jitter in stability.items():
                features[f'{name}_jitter'] = jitter
        
        return features


# =============================================================================
# 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
# =============================================================================
print("\nã€1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€‘")
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_subset = torch.utils.data.Subset(train_dataset, range(6000))
test_subset = torch.utils.data.Subset(test_dataset, range(300))

train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_subset, batch_size=1, shuffle=False)

print(f"  è¨“ç·´: {len(train_subset)}, ãƒ†ã‚¹ãƒˆ: {len(test_subset)}")


# =============================================================================
# 4. MiniViTå­¦ç¿’
# =============================================================================
print("\nã€2. MiniViTå­¦ç¿’ã€‘")
model = MiniViT(
    img_size=32, patch_size=4, in_chans=3, num_classes=10,
    d_model=128, depth=4, num_heads=4
)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

epochs = 15
for epoch in range(epochs):
    model.train()
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    
    if (epoch + 1) % 5 == 0:
        model.eval()
        correct = 0
        with torch.no_grad():
            for data, target in test_loader:
                pred = model(data).argmax(dim=1)
                correct += (pred == target).sum().item()
        print(f"  Epoch {epoch+1}: Accuracy = {100*correct/len(test_subset):.1f}%")

print(f"\n  MiniViTå­¦ç¿’å®Œäº†ï¼")


# =============================================================================
# 5. Transformer SNNè§£æ
# =============================================================================
print("\nã€3. Transformer SNNè§£æã€‘")

analyzer = TransformerSNNAnalyzer(timesteps=100, num_trials=8)

# ã‚µãƒ³ãƒ—ãƒ«è§£æ
sample_data, sample_target = next(iter(test_loader))
model.eval()

with torch.no_grad():
    output = model(sample_data)
    activations = model.get_all_activations()

print(f"\n  ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬: {output.argmax().item()} (æ­£è§£: {sample_target.item()})")

# Attention TTFSè§£æ
print(f"\n  ã€Attention TTFSè§£æã€‘")
for i, block in enumerate(model.blocks):
    attn_weights = block.attn.attention_weights
    if attn_weights is not None:
        analysis = analyzer.analyze_attention_ttfs(attn_weights)
        sync = analyzer.compute_attention_synchrony(attn_weights)
        print(f"    Block {i}: Incoming TTFS={analysis['mean_incoming']:.2f}, "
              f"Outgoing TTFS={analysis['mean_outgoing']:.2f}, Head Sync={sync:.3f}")


# =============================================================================
# 6. ç‰¹å¾´é‡æŠ½å‡º & ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥
# =============================================================================
print("\nã€4. ç‰¹å¾´é‡æŠ½å‡º & ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ã€‘")

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, classification_report

X_data = []
y_data = []

model.eval()
for i, (data, target) in enumerate(test_loader):
    if i >= 200:
        break
    
    features = analyzer.extract_features(model, data, noise_std=0.05)
    X_data.append(list(features.values()))
    
    with torch.no_grad():
        pred = model(data).argmax(dim=1).item()
    y_data.append(0 if pred == target.item() else 1)
    
    if (i + 1) % 50 == 0:
        print(f"  æŠ½å‡º: {i+1}/200")

X_data = np.array(X_data)
y_data = np.array(y_data)

print(f"\n  ç‰¹å¾´é‡æ•°: {X_data.shape[1]}")
print(f"  æ­£è§£: {sum(1 for y in y_data if y == 0)}, ä¸æ­£è§£: {sum(1 for y in y_data if y == 1)}")

# è¨“ç·´/æ¤œè¨¼åˆ†å‰²
split = 150
X_train, X_val = X_data[:split], X_data[split:]
y_train, y_val = y_data[:split], y_data[split:]

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

# åˆ†é¡å™¨å­¦ç¿’
clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
clf.fit(X_train_scaled, y_train)

# è©•ä¾¡
y_pred = clf.predict(X_val_scaled)
y_prob = clf.predict_proba(X_val_scaled)[:, 1]

print(f"\n  ã€æ¤œè¨¼çµæœã€‘")
print(classification_report(y_val, y_pred, target_names=['Correct', 'Wrong'], zero_division=0))
if len(set(y_val)) > 1:
    auc = roc_auc_score(y_val, y_prob)
    print(f"  AUC-ROC: {auc:.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦
feature_names = list(features.keys())
importances = clf.feature_importances_
indices = np.argsort(importances)[::-1]

print(f"\n  ã€Top 10 é‡è¦ç‰¹å¾´é‡ã€‘")
for i in range(min(10, len(feature_names))):
    idx = indices[i]
    print(f"    {i+1}. {feature_names[idx]}: {importances[idx]:.4f}")


# =============================================================================
# 7. å¯è¦–åŒ–
# =============================================================================
print("\nã€5. å¯è¦–åŒ–ã€‘")

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Attention TTFSæ¨ç§»
ax = axes[0, 0]
block_ttfs_in = []
block_ttfs_out = []
for i, block in enumerate(model.blocks):
    attn_weights = block.attn.attention_weights
    if attn_weights is not None:
        analysis = analyzer.analyze_attention_ttfs(attn_weights)
        block_ttfs_in.append(analysis['mean_incoming'])
        block_ttfs_out.append(analysis['mean_outgoing'])

ax.plot(range(len(block_ttfs_in)), block_ttfs_in, 'bo-', label='Incoming', linewidth=2)
ax.plot(range(len(block_ttfs_out)), block_ttfs_out, 'ro-', label='Outgoing', linewidth=2)
ax.set_xlabel('Block Index')
ax.set_ylabel('Mean TTFS')
ax.set_title('Attention TTFS Across Blocks')
ax.legend()
ax.grid(True, alpha=0.3)

# Head Synchrony
ax = axes[0, 1]
head_syncs = []
for i, block in enumerate(model.blocks):
    attn_weights = block.attn.attention_weights
    if attn_weights is not None:
        sync = analyzer.compute_attention_synchrony(attn_weights)
        head_syncs.append(sync)

ax.bar(range(len(head_syncs)), head_syncs, color='steelblue')
ax.set_xlabel('Block Index')
ax.set_ylabel('Head Synchrony')
ax.set_title('Multi-Head Attention Synchrony')

# ç‰¹å¾´é‡é‡è¦åº¦
ax = axes[1, 0]
top_n = 10
top_indices = indices[:top_n]
top_names = [feature_names[i][:20] for i in top_indices]
top_imp = importances[top_indices]
ax.barh(range(top_n), top_imp, color='forestgreen')
ax.set_yticks(range(top_n))
ax.set_yticklabels(top_names)
ax.set_xlabel('Importance')
ax.set_title('Top 10 Feature Importances (Transformer)')
ax.invert_yaxis()

# ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç¢ºç‡åˆ†å¸ƒ
ax = axes[1, 1]
correct_prob = [y_prob[i] for i in range(len(y_val)) if y_val[i] == 0]
wrong_prob = [y_prob[i] for i in range(len(y_val)) if y_val[i] == 1]
ax.hist(correct_prob, bins=15, alpha=0.7, label='Correct', color='green')
ax.hist(wrong_prob, bins=15, alpha=0.7, label='Wrong', color='red')
ax.set_xlabel('Hallucination Probability')
ax.set_ylabel('Count')
ax.set_title('Predicted Hallucination Probability')
ax.legend()

plt.tight_layout()
plt.savefig('transformer_snn_analysis.png', dpi=150, bbox_inches='tight')
print("  ä¿å­˜: transformer_snn_analysis.png")


# =============================================================================
# 8. ã¾ã¨ã‚
# =============================================================================
print("\n" + "=" * 70)
print("ğŸ§  Transformer SNN Analysis ã¾ã¨ã‚")
print("=" * 70)

print(f"""
ã€æ‰‹æ³•ã€‘
  - Attentioné‡ã¿ â†’ TTFSå¤‰æ› â†’ æ³¨ç›®å„ªå…ˆåº¦
  - Multi-HeadåŒæœŸåº¦ â†’ æ¦‚å¿µã®ä¸€è²«æ€§
  - å±¤åˆ¥ã‚¸ãƒƒã‚¿ãƒ¼ â†’ åˆ¤æ–­ã®å®‰å®šæ€§

ã€ä¸»è¦ç™ºè¦‹ã€‘
  - Attention TTFSã§ãƒˆãƒ¼ã‚¯ãƒ³é‡è¦åº¦ãŒå¯è¦–åŒ–å¯èƒ½
  - HeadåŒæœŸåº¦ãŒé«˜ã„ = è¤‡æ•°ã®è¦–ç‚¹ãŒä¸€è‡´ = ç¢ºä¿¡åº¦é«˜
  - æ·±ã„å±¤ã»ã©TTFSåˆ†æ•£ãŒå°ã•ã„å‚¾å‘

ã€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ã€‘
  - ç‰¹å¾´é‡æ•°: {len(feature_names)}
  - AUC-ROC: {auc:.4f} (Transformerç‰ˆ)
  
ã€é‡è¦ç‰¹å¾´é‡ã€‘
  - {feature_names[indices[0]]}: {importances[indices[0]]:.4f}
  - {feature_names[indices[1]]}: {importances[indices[1]]:.4f}
  - {feature_names[indices[2]]}: {importances[indices[2]]:.4f}

ã€LLMã¸ã®å±•æœ›ã€‘
  - GPT/Llamaã®ä¸­é–“å±¤ã«ã‚‚åŒæ§˜ã®æ‰‹æ³•ãŒé©ç”¨å¯èƒ½
  - Attentionå±¤ã®TTFSè§£æã§ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬
  - å®Ÿè£…ã¯HuggingFace Transformersã¨ã®çµ±åˆãŒå¿…è¦
""")

print("\nğŸš€ Transformer + SNN = AIæ€è€ƒã®å¯è¦–åŒ–é©å‘½ï¼")
print("=" * 70)
