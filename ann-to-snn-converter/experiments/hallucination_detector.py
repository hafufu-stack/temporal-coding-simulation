"""
SNN Hallucination Detector: ã‚¸ãƒƒã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¹åˆ†é¡å™¨
=================================================

Softmaxç¢ºç‡ã¨ã‚¹ãƒ‘ã‚¤ã‚¯ã‚¸ãƒƒã‚¿ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã¦
AIã®ã€Œè‡ªä¿¡éå‰°ãªèª¤ã‚Šã€ï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’æ¤œçŸ¥ã™ã‚‹ã€‚

ç™ºè¦‹: æ­£ã®ç›¸é–¢ r=0.22 ã‚’æ´»ç”¨
- é«˜ç¢ºä¿¡åº¦ + é«˜ã‚¸ãƒƒã‚¿ãƒ¼ = å±é™ºä¿¡å·ï¼ˆè‡ªä¿¡éå‰°ï¼‰
- é«˜ç¢ºä¿¡åº¦ + ä½ã‚¸ãƒƒã‚¿ãƒ¼ = å®‰å…¨ï¼ˆæœ¬å½“ã«ç¢ºä¿¡ï¼‰

Author: ã‚ãƒ¼ã‚‹ (Cell Activation)
Date: 2026-02-04
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, roc_curve
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']

print("=" * 70)
print("ğŸ” SNN Hallucination Detector: ã‚¸ãƒƒã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦è£œæ­£")
print("=" * 70)


# =============================================================================
# 1. ãƒ¢ãƒ‡ãƒ«å®šç¾©
# =============================================================================
class ResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_ch != out_ch:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_ch)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        return F.relu(out)


class SmallResNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(64, 64, 2)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)
        self.activations = {}
    
    def _make_layer(self, in_ch, out_ch, blocks, stride=1):
        layers = [ResBlock(in_ch, out_ch, stride)]
        for _ in range(1, blocks):
            layers.append(ResBlock(out_ch, out_ch))
        return nn.Sequential(*layers)
    
    def forward(self, x, save_act=False):
        x = F.relu(self.bn1(self.conv1(x)))
        if save_act: self.activations['conv1'] = x.clone()
        x = self.layer1(x)
        if save_act: self.activations['layer1'] = x.clone()
        x = self.layer2(x)
        if save_act: self.activations['layer2'] = x.clone()
        x = self.layer3(x)
        if save_act: self.activations['layer3'] = x.clone()
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)
    
    def get_activations(self, x):
        self.activations = {}
        _ = self.forward(x, save_act=True)
        return self.activations


# =============================================================================
# 2. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥å™¨
# =============================================================================
class HallucinationDetector:
    """SNNã‚¸ãƒƒã‚¿ãƒ¼ã«ã‚ˆã‚‹ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥"""
    
    def __init__(self, model, timesteps=100, num_trials=15):
        self.model = model
        self.timesteps = timesteps
        self.num_trials = num_trials
        
        # æ¤œçŸ¥é–¾å€¤ï¼ˆå¾Œã§å­¦ç¿’ï¼‰
        self.jitter_threshold = None
        self.combined_threshold = None
    
    def compute_ttfs(self, activation):
        """TTFSè¨ˆç®—"""
        ttfs = torch.full_like(activation, float(self.timesteps))
        active_mask = activation > 0
        if active_mask.any():
            max_act = activation.max()
            if max_act > 0:
                normalized = activation[active_mask] / max_act
                ttfs[active_mask] = self.timesteps * (1 - normalized)
        return ttfs
    
    def compute_jitter(self, x, noise_std=0.1):
        """ã‚¸ãƒƒã‚¿ãƒ¼è¨ˆç®—ï¼ˆè¤‡æ•°å›ãƒã‚¤ã‚ºä»˜åŠ ã§ç™ºç«ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®æºã‚Œã‚’æ¸¬å®šï¼‰"""
        all_ttfs = []
        
        self.model.eval()
        with torch.no_grad():
            for _ in range(self.num_trials):
                noisy_x = x + torch.randn_like(x) * noise_std
                noisy_x = torch.clamp(noisy_x, 0, 1)
                
                activations = self.model.get_activations(noisy_x)
                
                layer_ttfs = []
                for layer_name, act in activations.items():
                    ttfs = self.compute_ttfs(act)
                    layer_ttfs.append(ttfs.mean().item())
                
                all_ttfs.append(np.mean(layer_ttfs))
        
        return np.std(all_ttfs)  # ã‚¸ãƒƒã‚¿ãƒ¼ = ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—é–“ã®ã°ã‚‰ã¤ã
    
    def get_confidence(self, x):
        """Softmaxç¢ºä¿¡åº¦ã‚’å–å¾—"""
        self.model.eval()
        with torch.no_grad():
            output = self.model(x)
            probs = F.softmax(output, dim=1)
            confidence = probs.max().item()
            pred = output.argmax(dim=1).item()
        return confidence, pred
    
    def compute_risk_score(self, x, noise_std=0.1):
        """
        ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢è¨ˆç®—
        
        ä½ã‚¹ã‚³ã‚¢ = å®‰å…¨ï¼ˆé«˜ç¢ºä¿¡åº¦ + ä½ã‚¸ãƒƒã‚¿ãƒ¼ï¼‰
        é«˜ã‚¹ã‚³ã‚¢ = å±é™ºï¼ˆé«˜ç¢ºä¿¡åº¦ + é«˜ã‚¸ãƒƒã‚¿ãƒ¼ = è‡ªä¿¡éå‰°ï¼‰
        """
        confidence, pred = self.get_confidence(x)
        jitter = self.compute_jitter(x, noise_std)
        
        # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ = ã‚¸ãƒƒã‚¿ãƒ¼ Ã— ç¢ºä¿¡åº¦
        # é«˜ç¢ºä¿¡åº¦ã§é«˜ã‚¸ãƒƒã‚¿ãƒ¼ = æœ€ã‚‚å±é™º
        risk_score = jitter * confidence
        
        return {
            'confidence': confidence,
            'jitter': jitter,
            'risk_score': risk_score,
            'prediction': pred
        }
    
    def calibrate(self, dataloader, num_samples=100):
        """æ¤œçŸ¥é–¾å€¤ã‚’ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        print("\n  ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­...")
        
        correct_risks = []
        incorrect_risks = []
        
        for i, (data, target) in enumerate(dataloader):
            if i >= num_samples:
                break
            
            result = self.compute_risk_score(data)
            is_correct = result['prediction'] == target.item()
            
            if is_correct:
                correct_risks.append(result['risk_score'])
            else:
                incorrect_risks.append(result['risk_score'])
            
            if (i + 1) % 25 == 0:
                print(f"    å‡¦ç†: {i+1}/{num_samples}")
        
        # é–¾å€¤æ±ºå®š: æ­£è§£ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã‚’æ¡ç”¨
        if correct_risks:
            self.jitter_threshold = np.percentile(correct_risks, 95)
        
        print(f"\n  æ­£è§£ã‚µãƒ³ãƒ—ãƒ«: {len(correct_risks)}")
        print(f"  ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«: {len(incorrect_risks)}")
        print(f"  é–¾å€¤: {self.jitter_threshold:.4f}")
        
        return correct_risks, incorrect_risks
    
    def is_hallucination(self, x):
        """ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š"""
        if self.jitter_threshold is None:
            raise ValueError("å…ˆã«calibrateã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        result = self.compute_risk_score(x)
        is_risky = result['risk_score'] > self.jitter_threshold
        
        return {
            **result,
            'is_hallucination': is_risky,
            'threshold': self.jitter_threshold
        }


# =============================================================================
# 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
# =============================================================================
print("\nã€1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€‘")
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_subset = torch.utils.data.Subset(train_dataset, range(8000))
test_subset = torch.utils.data.Subset(test_dataset, range(500))
val_subset = torch.utils.data.Subset(test_dataset, range(500, 1000))

train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_subset, batch_size=1, shuffle=False)
val_loader = torch.utils.data.DataLoader(val_subset, batch_size=1, shuffle=False)

class_names = ['airplane', 'car', 'bird', 'cat', 'deer', 
               'dog', 'frog', 'horse', 'ship', 'truck']

print(f"  è¨“ç·´: {len(train_subset)}, ãƒ†ã‚¹ãƒˆ: {len(test_subset)}, æ¤œè¨¼: {len(val_subset)}")


# =============================================================================
# 4. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
# =============================================================================
print("\nã€2. SmallResNetå­¦ç¿’ã€‘")
model = SmallResNet(num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(15):
    model.train()
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    
    if (epoch + 1) % 5 == 0:
        model.eval()
        correct = 0
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
        acc = 100.0 * correct / len(test_subset)
        print(f"  Epoch {epoch+1}: Accuracy = {acc:.1f}%")

print(f"\n  ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†ï¼")


# =============================================================================
# 5. æ¤œçŸ¥å™¨ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# =============================================================================
print("\nã€3. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥å™¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€‘")

detector = HallucinationDetector(model, timesteps=100, num_trials=10)
correct_risks, incorrect_risks = detector.calibrate(test_loader, num_samples=100)


# =============================================================================
# 6. æ¤œè¨¼ã‚»ãƒƒãƒˆã§è©•ä¾¡
# =============================================================================
print("\nã€4. æ¤œè¨¼ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡ã€‘")

predictions = []
labels = []  # 1 = ä¸æ­£è§£ï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ‰±ã„ï¼‰
risk_scores = []
confidences = []

model.eval()
for i, (data, target) in enumerate(val_loader):
    if i >= 150:
        break
    
    result = detector.is_hallucination(data)
    is_correct = result['prediction'] == target.item()
    
    predictions.append(1 if result['is_hallucination'] else 0)
    labels.append(0 if is_correct else 1)  # ä¸æ­£è§£ã‚’1ã¨ã™ã‚‹
    risk_scores.append(result['risk_score'])
    confidences.append(result['confidence'])
    
    if (i + 1) % 50 == 0:
        print(f"  å‡¦ç†: {i+1}/150")

# è©•ä¾¡æŒ‡æ¨™
print(f"\n  è©•ä¾¡çµæœ:")
print(f"  {'-'*50}")

# æ··åŒè¡Œåˆ—
tp = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)
fp = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)
tn = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 0)
fn = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)

precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

print(f"  çœŸé™½æ€§ (æ­£ã—ãèª¤ã‚Šã‚’æ¤œçŸ¥): {tp}")
print(f"  å½é™½æ€§ (æ­£è§£ã‚’èª¤ã‚Šã¨åˆ¤å®š): {fp}")
print(f"  çœŸé™°æ€§ (æ­£ã—ãæ­£è§£ã¨åˆ¤å®š): {tn}")
print(f"  å½é™°æ€§ (èª¤ã‚Šã‚’è¦‹é€ƒã—): {fn}")
print(f"\n  é©åˆç‡ (Precision): {precision:.4f}")
print(f"  å†ç¾ç‡ (Recall): {recall:.4f}")
print(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")

# AUC-ROC
if len(set(labels)) > 1:
    try:
        auc = roc_auc_score(labels, risk_scores)
        print(f"  AUC-ROC: {auc:.4f}")
    except:
        auc = None
        print(f"  AUC-ROC: è¨ˆç®—ä¸å¯")
else:
    auc = None
    print(f"  AUC-ROC: å˜ä¸€ã‚¯ãƒ©ã‚¹ã®ãŸã‚è¨ˆç®—ä¸å¯")


# =============================================================================
# 7. å¯è¦–åŒ–
# =============================================================================
print("\nã€5. å¯è¦–åŒ–ã€‘")

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
ax = axes[0, 0]
correct_risks_val = [r for r, l in zip(risk_scores, labels) if l == 0]
incorrect_risks_val = [r for r, l in zip(risk_scores, labels) if l == 1]
ax.hist(correct_risks_val, bins=20, alpha=0.7, label='Correct', color='green')
ax.hist(incorrect_risks_val, bins=20, alpha=0.7, label='Incorrect', color='red')
ax.axvline(detector.jitter_threshold, color='blue', linestyle='--', label=f'Threshold={detector.jitter_threshold:.3f}')
ax.set_xlabel('Risk Score (Jitter Ã— Confidence)')
ax.set_ylabel('Count')
ax.set_title('Risk Score Distribution')
ax.legend()

# ROCæ›²ç·š
ax = axes[0, 1]
if len(set(labels)) > 1:
    fpr, tpr, _ = roc_curve(labels, risk_scores)
    ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC={auc:.3f}' if auc else 'AUC=N/A')
    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('ROC Curve')
    ax.legend()
else:
    ax.text(0.5, 0.5, 'Not enough data', ha='center', va='center')
    ax.set_title('ROC Curve (N/A)')

# ç¢ºä¿¡åº¦ vs ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢
ax = axes[1, 0]
colors = ['green' if l == 0 else 'red' for l in labels]
ax.scatter(confidences, risk_scores, c=colors, alpha=0.7)
ax.axhline(detector.jitter_threshold, color='blue', linestyle='--', label='Threshold')
ax.set_xlabel('Softmax Confidence')
ax.set_ylabel('Risk Score')
ax.set_title('Confidence vs Risk Score')
ax.legend()

# æ··åŒè¡Œåˆ—
ax = axes[1, 1]
cm = [[tn, fp], [fn, tp]]
im = ax.imshow(cm, cmap='Blues')
ax.set_xticks([0, 1])
ax.set_yticks([0, 1])
ax.set_xticklabels(['Predicted\nCorrect', 'Predicted\nHallucination'])
ax.set_yticklabels(['Actual\nCorrect', 'Actual\nWrong'])
ax.set_title('Confusion Matrix')
for i in range(2):
    for j in range(2):
        ax.text(j, i, str(cm[i][j]), ha='center', va='center', fontsize=16)
plt.colorbar(im, ax=ax)

plt.tight_layout()
plt.savefig('hallucination_detector_results.png', dpi=150, bbox_inches='tight')
print("  ä¿å­˜: hallucination_detector_results.png")


# =============================================================================
# 8. ã¾ã¨ã‚
# =============================================================================
print("\n" + "=" * 70)
print("ğŸ” SNN Hallucination Detector ã¾ã¨ã‚")
print("=" * 70)

print(f"""
ã€æ‰‹æ³•ã€‘
  ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ = ã‚¸ãƒƒã‚¿ãƒ¼ Ã— Softmaxç¢ºä¿¡åº¦
  
  é«˜ç¢ºä¿¡åº¦ + é«˜ã‚¸ãƒƒã‚¿ãƒ¼ = ã€Œè‡ªä¿¡éå‰°ãªèª¤ã‚Šã€= ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³
  
ã€è©•ä¾¡çµæœã€‘
  - Precision: {precision:.4f}
  - Recall: {recall:.4f}
  - F1: {f1:.4f}
  - é–¾å€¤: {detector.jitter_threshold:.4f}

ã€è§£é‡ˆã€‘
  - SNNã®ã‚¸ãƒƒã‚¿ãƒ¼ã¯ã€Œåˆ¤æ–­ã®å®‰å®šæ€§ã€ã‚’è¡¨ã™
  - Softmaxç¢ºä¿¡åº¦ã ã‘ã§ã¯è¦‹æŠœã‘ãªã„èª¤ã‚Šã‚’æ¤œçŸ¥å¯èƒ½
  - ã€Œç¢ºä¿¡æº€ã€…ã ãŒä¸å®‰å®šã€= AIéä¿¡ã®è­¦å‘Š

ã€ä»Šå¾Œã®èª²é¡Œã€‘
  1. ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼
  2. æœ€é©ãªé–¾å€¤ã®è‡ªå‹•èª¿æ•´
  3. Transformer/LLMã¸ã®å¿œç”¨
  4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œçŸ¥API
""")

print("\nğŸš€ SNN = ã€ŒAIã®å˜˜ç™ºè¦‹å™¨ã€ï¼éä¿¡ã‚’è¦‹æŠœãæ–°æŠ€è¡“")
print("=" * 70)
