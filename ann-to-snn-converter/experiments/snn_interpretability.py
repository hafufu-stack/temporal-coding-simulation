"""
SNN AI Interpretability: Dynamic Analysis of ANN Black Box
============================================================

SNNã‚’ã€Œæ™‚é–“çš„é¡•å¾®é¡ã€ã¨ã—ã¦ä½¿ç”¨ã—ã€ANNã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è§£å‰–ã™ã‚‹ã€‚

3ã¤ã®è§£ææ‰‹æ³•:
1. TTFS (Time-to-First-Spike): æ€è€ƒé †åºã®å¯è¦–åŒ–
2. Neural Synchrony: æ¦‚å¿µçµåˆï¼ˆåŒæœŸï¼‰ã®ç™ºè¦‹  
3. Spike Stability: ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ï¼ˆã‚¸ãƒƒã‚¿ãƒ¼è§£æï¼‰

Author: ã‚ãƒ¼ã‚‹ (Cell Activation)
Date: 2026-02-04
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']

print("=" * 70)
print("ğŸ§  SNN AI Interpretability: ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®æ™‚é–“çš„è§£å‰–")
print("=" * 70)


# =============================================================================
# 1. ã‚·ãƒ³ãƒ—ãƒ«ãªCNNãƒ¢ãƒ‡ãƒ«ï¼ˆè§£æå¯¾è±¡ï¼‰
# =============================================================================
class SimpleCNN(nn.Module):
    """è§£æå¯¾è±¡ã®ã‚·ãƒ³ãƒ—ãƒ«ãªCNN"""
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 4 * 4, 256)
        self.fc2 = nn.Linear(256, num_classes)
    
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16
        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8
        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
    
    def get_layer_activations(self, x):
        """å„å±¤ã®æ´»æ€§åŒ–å€¤ã‚’å–å¾—"""
        activations = {}
        
        x = self.conv1(x)
        x_relu = F.relu(x)
        activations['conv1'] = x_relu.clone()
        x = self.pool(x_relu)
        
        x = self.conv2(x)
        x_relu = F.relu(x)
        activations['conv2'] = x_relu.clone()
        x = self.pool(x_relu)
        
        x = self.conv3(x)
        x_relu = F.relu(x)
        activations['conv3'] = x_relu.clone()
        x = self.pool(x_relu)
        
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x_relu = F.relu(x)
        activations['fc1'] = x_relu.clone()
        
        activations['output'] = self.fc2(x_relu)
        
        return activations


# =============================================================================
# 2. SNNå¤‰æ› & TTFSè¨ˆç®—
# =============================================================================
class SNNAnalyzer:
    """SNNå¤‰æ›ã¨æ™‚é–“çš„è§£æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, alpha=2.0, timesteps=50):
        self.alpha = alpha
        self.timesteps = timesteps
    
    def compute_ttfs(self, activation, threshold):
        """
        Time-to-First-Spike (TTFS) ã‚’è¨ˆç®—
        
        é«˜ã„æ´»æ€§åŒ–å€¤ â†’ æ—©ã„ç™ºç«ï¼ˆå°ã•ã„TTFSï¼‰
        ä½ã„æ´»æ€§åŒ–å€¤ â†’ é…ã„ç™ºç«ï¼ˆå¤§ãã„TTFSï¼‰
        """
        # æ´»æ€§åŒ–å€¤ãŒ0ä»¥ä¸‹ã®å ´åˆã¯ç™ºç«ã—ãªã„ï¼ˆæœ€å¤§ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        ttfs = torch.full_like(activation, float(self.timesteps))
        
        # ç™ºç«ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®TTFSè¨ˆç®—
        # TTFS âˆ 1/activation (é«˜ã„æ´»æ€§åŒ– â†’ æ—©ã„ç™ºç«)
        active_mask = activation > 0
        if active_mask.any():
            # æ­£è¦åŒ–ã—ã¦0ï½timestepsã«ã‚¹ã‚±ãƒ¼ãƒ«
            max_act = activation.max()
            if max_act > 0:
                normalized = activation[active_mask] / max_act
                # é«˜ã„æ´»æ€§åŒ– â†’ å°ã•ã„TTFS
                ttfs[active_mask] = self.timesteps * (1 - normalized)
        
        return ttfs
    
    def analyze_layer_ttfs(self, activations):
        """å„å±¤ã®TTFSã‚’è¨ˆç®—"""
        ttfs_results = {}
        
        for layer_name, act in activations.items():
            if layer_name == 'output':
                continue
                
            # é–¾å€¤è¨ˆç®—ï¼ˆÎ±=2.0å…¬å¼ï¼‰
            threshold = self.alpha * act.max().item()
            
            # TTFSè¨ˆç®—
            ttfs = self.compute_ttfs(act, threshold)
            
            ttfs_results[layer_name] = {
                'ttfs': ttfs,
                'activation': act,
                'threshold': threshold,
                'mean_ttfs': ttfs.mean().item(),
                'min_ttfs': ttfs.min().item()
            }
        
        return ttfs_results
    
    def compute_synchrony(self, ttfs_layer, tolerance=2.0):
        """
        Neural Synchronyï¼ˆåŒæœŸç™ºç«ãƒšã‚¢ï¼‰ã‚’æ¤œå‡º
        
        åŒã˜ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆtoleranceå†…ï¼‰ã§ç™ºç«ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒšã‚¢ã‚’æ¤œå‡º
        """
        if len(ttfs_layer.shape) == 4:
            # Convå±¤: (B, C, H, W) -> flatten
            ttfs_flat = ttfs_layer.view(ttfs_layer.size(0), -1)
        else:
            ttfs_flat = ttfs_layer
        
        synchrony_matrix = torch.zeros(ttfs_flat.size(1), ttfs_flat.size(1))
        
        for i in range(ttfs_flat.size(1)):
            for j in range(i+1, ttfs_flat.size(1)):
                # ç™ºç«ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®å·®
                diff = torch.abs(ttfs_flat[0, i] - ttfs_flat[0, j])
                if diff < tolerance:
                    synchrony_matrix[i, j] = 1.0
                    synchrony_matrix[j, i] = 1.0
        
        return synchrony_matrix
    
    def compute_spike_stability(self, model, x, num_trials=10, noise_std=0.05):
        """
        Spike Stabilityè§£æï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ç”¨ï¼‰
        
        å…¥åŠ›ã«ãƒã‚¤ã‚ºã‚’åŠ ãˆã¦è¤‡æ•°å›æ¨è«–ã—ã€ç™ºç«ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®æºã‚Œï¼ˆã‚¸ãƒƒã‚¿ãƒ¼ï¼‰ã‚’æ¸¬å®š
        """
        stability_results = {}
        all_ttfs = defaultdict(list)
        
        for trial in range(num_trials):
            # ãƒã‚¤ã‚ºä»˜åŠ 
            noisy_x = x + torch.randn_like(x) * noise_std
            noisy_x = torch.clamp(noisy_x, 0, 1)
            
            # æ´»æ€§åŒ–å–å¾—
            activations = model.get_layer_activations(noisy_x)
            
            # TTFSè¨ˆç®—
            ttfs_results = self.analyze_layer_ttfs(activations)
            
            for layer_name, result in ttfs_results.items():
                all_ttfs[layer_name].append(result['ttfs'])
        
        # ã‚¸ãƒƒã‚¿ãƒ¼è¨ˆç®—ï¼ˆæ¨™æº–åå·®ï¼‰
        for layer_name, ttfs_list in all_ttfs.items():
            stacked = torch.stack(ttfs_list)
            jitter = stacked.std(dim=0)  # è©¦è¡Œé–“ã®æ¨™æº–åå·®
            
            stability_results[layer_name] = {
                'jitter_mean': jitter.mean().item(),
                'jitter_max': jitter.max().item(),
                'jitter_map': jitter
            }
        
        return stability_results


# =============================================================================
# 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
# =============================================================================
print("\nã€1. CIFAR-10ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€‘")
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

try:
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
except:
    print("  CIFAR-10ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# ã‚µãƒ–ã‚»ãƒƒãƒˆä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ï¼‰
train_subset = torch.utils.data.Subset(train_dataset, range(5000))
test_subset = torch.utils.data.Subset(test_dataset, range(500))

train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_subset, batch_size=1, shuffle=False)

class_names = ['airplane', 'car', 'bird', 'cat', 'deer', 
               'dog', 'frog', 'horse', 'ship', 'truck']

print(f"  è¨“ç·´: {len(train_subset)}, ãƒ†ã‚¹ãƒˆ: {len(test_subset)}")


# =============================================================================
# 4. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
# =============================================================================
print("\nã€2. CNNãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€‘")
model = SimpleCNN(num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    # ç²¾åº¦ç¢ºèª
    if (epoch + 1) % 5 == 0:
        model.eval()
        correct = 0
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
        acc = 100.0 * correct / len(test_subset)
        print(f"  Epoch {epoch+1}: Accuracy = {acc:.1f}%")

print(f"\n  ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†ï¼")


# =============================================================================
# 5. TTFSè§£æï¼ˆæ€è€ƒé †åºã®å¯è¦–åŒ–ï¼‰
# =============================================================================
print("\nã€3. TTFSè§£æ - æ€è€ƒé †åºã®å¯è¦–åŒ–ã€‘")

analyzer = SNNAnalyzer(alpha=2.0, timesteps=50)

# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’å–å¾—
sample_data, sample_target = next(iter(test_loader))
sample_class = class_names[sample_target.item()]

print(f"  ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚¯ãƒ©ã‚¹: {sample_class}")

# æ´»æ€§åŒ–å–å¾—
model.eval()
with torch.no_grad():
    activations = model.get_layer_activations(sample_data)

# TTFSè¨ˆç®—
ttfs_results = analyzer.analyze_layer_ttfs(activations)

print(f"\n  å„å±¤ã®TTFSçµ±è¨ˆ:")
print(f"  {'-'*60}")
print(f"  {'å±¤å':<10} | {'å¹³å‡TTFS':>12} | {'æœ€å°TTFS':>12} | {'é–¾å€¤':>10}")
print(f"  {'-'*60}")
for layer_name, result in ttfs_results.items():
    print(f"  {layer_name:<10} | {result['mean_ttfs']:>12.2f} | {result['min_ttfs']:>12.2f} | {result['threshold']:>10.2f}")


# =============================================================================
# 6. Neural Synchronyè§£æï¼ˆæ¦‚å¿µçµåˆï¼‰
# =============================================================================
print("\nã€4. Neural Synchronyè§£æ - æ¦‚å¿µçµåˆã®ç™ºè¦‹ã€‘")

# FCå±¤ã§ã®åŒæœŸæ¤œå‡º
fc1_ttfs = ttfs_results['fc1']['ttfs']
sync_matrix = analyzer.compute_synchrony(fc1_ttfs, tolerance=3.0)

# åŒæœŸãƒšã‚¢æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
num_sync_pairs = (sync_matrix.sum() / 2).int().item()
total_pairs = (fc1_ttfs.numel() * (fc1_ttfs.numel() - 1)) // 2
sync_ratio = num_sync_pairs / total_pairs * 100

print(f"  FC1å±¤ã§ã®åŒæœŸãƒšã‚¢: {num_sync_pairs}/{total_pairs} ({sync_ratio:.2f}%)")
print(f"  â†’ åŒæœŸãƒšã‚¢ã¯ã€Œæ¦‚å¿µã®å¡Šã€ã‚’å½¢æˆã—ã¦ã„ã‚‹å¯èƒ½æ€§")


# =============================================================================
# 7. Spike Stabilityè§£æï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥PoCï¼‰
# =============================================================================
print("\nã€5. Spike Stabilityè§£æ - ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥PoCã€‘")

# æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã§ã®ã‚¸ãƒƒã‚¿ãƒ¼
correct_samples = []
incorrect_samples = []

with torch.no_grad():
    for i, (data, target) in enumerate(test_loader):
        if i >= 50:  # æœ€åˆã®50ã‚µãƒ³ãƒ—ãƒ«ã§è§£æ
            break
        
        output = model(data)
        pred = output.argmax(dim=1)
        
        # Spike Stabilityè¨ˆç®—
        stability = analyzer.compute_spike_stability(model, data, num_trials=10, noise_std=0.05)
        
        avg_jitter = np.mean([s['jitter_mean'] for s in stability.values()])
        
        if pred.item() == target.item():
            correct_samples.append(avg_jitter)
        else:
            incorrect_samples.append(avg_jitter)

print(f"\n  çµæœ:")
print(f"  {'-'*50}")
if correct_samples:
    correct_jitter = np.mean(correct_samples)
    print(f"  æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡ã‚¸ãƒƒã‚¿ãƒ¼: {correct_jitter:.4f} (n={len(correct_samples)})")
if incorrect_samples:
    incorrect_jitter = np.mean(incorrect_samples)
    print(f"  ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡ã‚¸ãƒƒã‚¿ãƒ¼: {incorrect_jitter:.4f} (n={len(incorrect_samples)})")
else:
    print(f"  ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«: ãªã—ï¼ˆå…¨ã¦æ­£è§£ï¼‰")
    incorrect_jitter = None

print(f"\n  ã€è§£é‡ˆã€‘")
if incorrect_samples and len(incorrect_samples) > 0:
    if incorrect_jitter > correct_jitter:
        ratio = incorrect_jitter / correct_jitter
        print(f"  âœ… ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¸ãƒƒã‚¿ãƒ¼ãŒ {ratio:.2f}x å¤§ãã„ï¼")
        print(f"  â†’ ã‚¹ãƒ‘ã‚¤ã‚¯ä¸å®‰å®šæ€§ãŒãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®æŒ‡æ¨™ã«ãªã‚‹å¯èƒ½æ€§")
    else:
        print(f"  âš ï¸ ã‚¸ãƒƒã‚¿ãƒ¼å·®ãŒå°ã•ã„ - ã‚ˆã‚Šè©³ç´°ãªè§£æãŒå¿…è¦")
else:
    print(f"  â†’ ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ãŒå°‘ãªã„ãŸã‚ã€ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ¤œè¨¼ãŒå¿…è¦")


# =============================================================================
# 8. å¯è¦–åŒ–
# =============================================================================
print("\nã€6. å¯è¦–åŒ–ã€‘")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# å…¥åŠ›ç”»åƒ
ax = axes[0, 0]
img = sample_data[0].permute(1, 2, 0).numpy()
img = (img - img.min()) / (img.max() - img.min())  # æ­£è¦åŒ–
ax.imshow(img)
ax.set_title(f'Input: {sample_class}', fontsize=12)
ax.axis('off')

# Conv1 TTFS
ax = axes[0, 1]
ttfs_conv1 = ttfs_results['conv1']['ttfs'][0].mean(dim=0).numpy()
im = ax.imshow(ttfs_conv1, cmap='hot', vmin=0, vmax=50)
ax.set_title('Conv1 TTFS (Thought Priority)', fontsize=12)
plt.colorbar(im, ax=ax, label='TTFS')

# Conv2 TTFS
ax = axes[0, 2]
ttfs_conv2 = ttfs_results['conv2']['ttfs'][0].mean(dim=0).numpy()
im = ax.imshow(ttfs_conv2, cmap='hot', vmin=0, vmax=50)
ax.set_title('Conv2 TTFS', fontsize=12)
plt.colorbar(im, ax=ax, label='TTFS')

# Conv3 TTFS
ax = axes[1, 0]
ttfs_conv3 = ttfs_results['conv3']['ttfs'][0].mean(dim=0).numpy()
im = ax.imshow(ttfs_conv3, cmap='hot', vmin=0, vmax=50)
ax.set_title('Conv3 TTFS', fontsize=12)
plt.colorbar(im, ax=ax, label='TTFS')

# FC1åŒæœŸè¡Œåˆ—
ax = axes[1, 1]
im = ax.imshow(sync_matrix[:50, :50].numpy(), cmap='Blues')
ax.set_title('FC1 Synchrony (50x50)', fontsize=12)
ax.set_xlabel('Neuron i')
ax.set_ylabel('Neuron j')
plt.colorbar(im, ax=ax, label='Sync')

# ã‚¸ãƒƒã‚¿ãƒ¼æ¯”è¼ƒ
ax = axes[1, 2]
if correct_samples and incorrect_samples:
    bp = ax.boxplot([correct_samples, incorrect_samples], 
                    labels=['Correct', 'Incorrect'],
                    patch_artist=True)
    bp['boxes'][0].set_facecolor('lightgreen')
    bp['boxes'][1].set_facecolor('lightcoral')
    ax.set_ylabel('Mean Jitter')
    ax.set_title('Spike Stability: Jitter Comparison', fontsize=12)
else:
    ax.bar(['Correct'], [np.mean(correct_samples) if correct_samples else 0], color='lightgreen')
    ax.set_ylabel('Mean Jitter')
    ax.set_title('Spike Stability (Only Correct)', fontsize=12)

plt.tight_layout()
plt.savefig('snn_interpretability_analysis.png', dpi=150, bbox_inches='tight')
print("  ä¿å­˜: snn_interpretability_analysis.png")


# =============================================================================
# 9. ã¾ã¨ã‚
# =============================================================================
print("\n" + "=" * 70)
print("ğŸ“Š SNN AI Interpretability å®Ÿé¨“ã¾ã¨ã‚")
print("=" * 70)

print("""
ã€TTFSè§£æï¼ˆæ€è€ƒé †åºï¼‰ã€‘
  - å„å±¤ã§ã€Œä½•ã‚’æœ€åˆã«è¦‹ãŸã‹ã€ãŒå¯è¦–åŒ–å¯èƒ½
  - ä½ã„TTFSå€¤ = AIãŒé‡è¦è¦–ã—ã¦ã„ã‚‹ç‰¹å¾´
  - å±¤ãŒæ·±ããªã‚‹ã»ã©æŠ½è±¡çš„ãªã€Œæ¦‚å¿µã€ã®å„ªå…ˆåº¦ã‚’åæ˜ 

ã€Neural Synchronyï¼ˆæ¦‚å¿µçµåˆï¼‰ã€‘
  - åŒæœŸç™ºç«ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒšã‚¢ã‚’æ¤œå‡º
  - åŒæœŸãƒšã‚¢ = ã€Œæ„å‘³ã®å¡Šã€ã‚’å½¢æˆã™ã‚‹ç¥çµŒç¾¤
  - ã‚¯ãƒ©ã‚¹ã«ã‚ˆã£ã¦ç•°ãªã‚‹åŒæœŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯èƒ½æ€§

ã€Spike Stabilityï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ï¼‰ã€‘
  - ãƒã‚¤ã‚ºæ‘‚å‹•ä¸‹ã§ã®ç™ºç«ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®æºã‚Œã‚’æ¸¬å®š
  - é«˜ã„ã‚¸ãƒƒã‚¿ãƒ¼ = ä¸å®‰å®šãªåˆ¤æ–­ = ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®äºˆå…†
  - ã€Œè‡ªä¿¡æº€ã€…ã«å˜˜ã‚’ã¤ãã€AIã‚’è¦‹æŠœãæ–°æ‰‹æ³•

ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘
  1. ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆResNetï¼‰ã§ã®æ¤œè¨¼
  2. ã‚¯ãƒ©ã‚¹åˆ¥TTFS/Synchronyãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
  3. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ã®é–¾å€¤æœ€é©åŒ–
  4. LLMï¼ˆTransformerï¼‰ã¸ã®å¿œç”¨
""")

print("\nğŸš€ SNN = ã€ŒAIã®è„³æ³¢è¨ˆã€ã¨ã—ã¦æ©Ÿèƒ½ï¼")
print("=" * 70)
