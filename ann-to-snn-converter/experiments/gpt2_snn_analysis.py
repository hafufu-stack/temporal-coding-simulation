"""
GPT-2 SNN Analysis: HuggingFaceçµ±åˆ
====================================

HuggingFaceã®GPT-2ãƒ¢ãƒ‡ãƒ«ã®ä¸­é–“å±¤ã‚’SNNç‰¹å¾´é‡ã§è§£æã€‚
Attentionå±¤ã®TTFS/Synchronyã§ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ã‚’è©¦ã¿ã‚‹ã€‚

Author: ã‚ãƒ¼ã‚‹ (Cell Activation)
Date: 2026-02-04
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']

print("=" * 70)
print("ğŸ¤– GPT-2 SNN Analysis: HuggingFaceçµ±åˆ")
print("=" * 70)


# =============================================================================
# 1. HuggingFace GPT-2 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# =============================================================================
print("\nã€1. GPT-2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã€‘")

try:
    from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config
    print("  âœ… Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿æˆåŠŸ")
except ImportError:
    print("  âŒ TransformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("     pip install transformers")
    exit(1)

# å°å‹GPT-2ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆdistilgpt2 = 82M paramsï¼‰
model_name = "distilgpt2"
print(f"  ãƒ¢ãƒ‡ãƒ«: {model_name}")

tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True, output_hidden_states=True)
model.eval()

print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
print(f"  å±¤æ•°: {model.config.n_layer}")
print(f"  ãƒ˜ãƒƒãƒ‰æ•°: {model.config.n_head}")


# =============================================================================
# 2. GPT-2ç”¨SNNè§£æã‚¯ãƒ©ã‚¹
# =============================================================================
class GPT2SNNAnalyzer:
    """GPT-2ã®ä¸­é–“å±¤ã‚’SNNç‰¹å¾´é‡ã§è§£æ"""
    
    def __init__(self, model, tokenizer, timesteps=100, num_trials=5):
        self.model = model
        self.tokenizer = tokenizer
        self.timesteps = timesteps
        self.num_trials = num_trials
    
    def compute_ttfs(self, activation):
        """TTFSè¨ˆç®—ï¼ˆé«˜ã„æ´»æ€§åŒ– â†’ æ—©ã„ã‚¹ãƒ‘ã‚¤ã‚¯ï¼‰"""
        ttfs = torch.full_like(activation, float(self.timesteps))
        active_mask = activation > 0
        if active_mask.any():
            max_act = activation.max()
            if max_act > 0:
                normalized = activation[active_mask] / max_act
                ttfs[active_mask] = self.timesteps * (1 - normalized)
        return ttfs
    
    def analyze_attention(self, attention_weights):
        """
        Attentioné‡ã¿ã®SNNè§£æ
        
        attention_weights: tuple of (batch, heads, seq, seq) for each layer
        """
        results = []
        
        for layer_idx, attn in enumerate(attention_weights):
            # attn: (batch, heads, seq_len, seq_len)
            attn = attn.detach()
            
            # å„ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨ç›®åº¦ï¼ˆincoming attentionï¼‰
            incoming = attn.mean(dim=1).mean(dim=1)  # (batch, seq_len)
            
            # TTFSå¤‰æ›
            ttfs = self.compute_ttfs(incoming)
            
            # Headé–“ã®åŒæœŸåº¦ï¼ˆåŒã˜å ´æ‰€ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ï¼‰
            head_agreement = self._compute_head_sync(attn)
            
            results.append({
                'layer': layer_idx,
                'ttfs_mean': ttfs.mean().item(),
                'ttfs_std': ttfs.std().item(),
                'ttfs_min': ttfs.min().item(),
                'head_sync': head_agreement,
                'attention_entropy': self._attention_entropy(attn)
            })
        
        return results
    
    def _compute_head_sync(self, attn):
        """ãƒ˜ãƒƒãƒ‰é–“ã®åŒæœŸåº¦ã‚’è¨ˆç®—"""
        # attn: (batch, heads, seq, seq)
        num_heads = attn.size(1)
        if num_heads < 2:
            return 1.0
        
        # å„ãƒ˜ãƒƒãƒ‰ã®max attentionä½ç½®
        max_pos = attn.argmax(dim=-1)  # (batch, heads, seq)
        
        # ãƒ˜ãƒƒãƒ‰é–“ã®ä¸€è‡´ç‡
        sync_count = 0
        total = 0
        for i in range(num_heads):
            for j in range(i+1, num_heads):
                agreement = (max_pos[:, i] == max_pos[:, j]).float().mean()
                sync_count += agreement.item()
                total += 1
        
        return sync_count / total if total > 0 else 1.0
    
    def _attention_entropy(self, attn):
        """Attentionã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆåˆ†æ•£åº¦ï¼‰"""
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = æ³¨æ„ãŒåˆ†æ•£ = ç¢ºä¿¡åº¦ä½
        attn_flat = attn.mean(dim=1)  # (batch, seq, seq)
        entropy = -(attn_flat * torch.log(attn_flat + 1e-8)).sum(dim=-1).mean()
        return entropy.item()
    
    def analyze_hidden_states(self, hidden_states):
        """éš ã‚ŒçŠ¶æ…‹ã®SNNè§£æ"""
        results = []
        
        for layer_idx, hidden in enumerate(hidden_states):
            hidden = hidden.detach()
            
            # æ´»æ€§åŒ–çµ±è¨ˆ
            mean_act = hidden.mean().item()
            std_act = hidden.std().item()
            
            # TTFSè¨ˆç®—
            ttfs = self.compute_ttfs(F.relu(hidden))  # ReLUé©ç”¨ã—ã¦ã‹ã‚‰TTFS
            
            results.append({
                'layer': layer_idx,
                'mean_activation': mean_act,
                'std_activation': std_act,
                'ttfs_mean': ttfs.mean().item(),
                'sparsity': (hidden <= 0).float().mean().item()
            })
        
        return results
    
    def compute_generation_stability(self, prompt, num_trials=5, max_length=20, temperature=1.0):
        """ç”Ÿæˆã®å®‰å®šæ€§ã‚’æ¸¬å®š"""
        self.model.eval()
        
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        generations = []
        attention_patterns = []
        
        with torch.no_grad():
            for _ in range(num_trials):
                output = self.model.generate(
                    input_ids,
                    max_length=max_length,
                    do_sample=True,
                    temperature=temperature,
                    output_attentions=True,
                    return_dict_in_generate=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                generated_text = self.tokenizer.decode(output.sequences[0], skip_special_tokens=True)
                generations.append(generated_text)
        
        # ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆä½ã„ = å®‰å®šï¼‰
        unique_generations = len(set(generations))
        stability = 1.0 - (unique_generations - 1) / max(num_trials - 1, 1)
        
        return {
            'generations': generations,
            'unique_count': unique_generations,
            'stability_score': stability
        }
    
    def extract_full_features(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å…¨ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        self.model.eval()
        
        input_ids = self.tokenizer.encode(text, return_tensors='pt')
        
        with torch.no_grad():
            outputs = self.model(input_ids, output_attentions=True, output_hidden_states=True)
        
        features = {}
        
        # å‡ºåŠ›ç¢ºç‡
        logits = outputs.logits[0, -1]  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬
        probs = F.softmax(logits, dim=-1)
        
        features['top_prob'] = probs.max().item()
        features['output_entropy'] = -(probs * torch.log(probs + 1e-8)).sum().item()
        
        # Attentionè§£æ
        attn_results = self.analyze_attention(outputs.attentions)
        for res in attn_results:
            layer = res['layer']
            features[f'layer{layer}_attn_ttfs'] = res['ttfs_mean']
            features[f'layer{layer}_head_sync'] = res['head_sync']
            features[f'layer{layer}_attn_entropy'] = res['attention_entropy']
        
        # Hidden statesè§£æ
        hidden_results = self.analyze_hidden_states(outputs.hidden_states)
        for res in hidden_results:
            layer = res['layer']
            features[f'layer{layer}_hidden_ttfs'] = res['ttfs_mean']
            features[f'layer{layer}_hidden_sparsity'] = res['sparsity']
        
        return features


# =============================================================================
# 3. GPT-2è§£æå®Ÿè¡Œ
# =============================================================================
print("\nã€2. GPT-2 SNNè§£æã€‘")

analyzer = GPT2SNNAnalyzer(model, tokenizer)

# ãƒ†ã‚¹ãƒˆæ–‡
test_prompts = [
    "The capital of France is",
    "2 + 2 equals",
    "The meaning of life is",  # ã‚ˆã‚Šæ›–æ˜§
    "asdfghjkl qwerty",  # ç„¡æ„å‘³
]

print("\n  å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è§£æ:")
print("  " + "-" * 60)

all_features = []
for prompt in test_prompts:
    features = analyzer.extract_full_features(prompt)
    all_features.append(features)
    
    print(f"\n  ğŸ“ '{prompt[:30]}...'")
    print(f"     Topç¢ºç‡: {features['top_prob']:.4f}")
    print(f"     å‡ºåŠ›ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {features['output_entropy']:.2f}")
    print(f"     Layer0 Attn TTFS: {features['layer0_attn_ttfs']:.2f}")
    print(f"     Layer0 HeadåŒæœŸ: {features['layer0_head_sync']:.3f}")


# =============================================================================
# 4. ç”Ÿæˆå®‰å®šæ€§ãƒ†ã‚¹ãƒˆ
# =============================================================================
print("\nã€3. ç”Ÿæˆå®‰å®šæ€§ãƒ†ã‚¹ãƒˆã€‘")

stability_results = []
for prompt in test_prompts[:3]:  # æœ€åˆã®3ã¤ã ã‘
    result = analyzer.compute_generation_stability(prompt, num_trials=5, max_length=25)
    stability_results.append({
        'prompt': prompt,
        'stability': result['stability_score'],
        'unique': result['unique_count']
    })
    
    print(f"\n  ğŸ“ '{prompt[:25]}...'")
    print(f"     å®‰å®šæ€§ã‚¹ã‚³ã‚¢: {result['stability_score']:.2f}")
    print(f"     ãƒ¦ãƒ‹ãƒ¼ã‚¯ç”Ÿæˆæ•°: {result['unique_count']}/5")
    print(f"     ã‚µãƒ³ãƒ—ãƒ«: {result['generations'][0][:50]}...")


# =============================================================================
# 5. å±¤ã”ã¨ã®TTFSæ¨ç§»åˆ†æ
# =============================================================================
print("\nã€4. å±¤ã”ã¨ã®TTFSæ¨ç§»ã€‘")

# æ„å‘³ã®ã‚ã‚‹æ–‡ vs ç„¡æ„å‘³ãªæ–‡
meaningful_prompt = "The quick brown fox jumps over the lazy dog"
meaningless_prompt = "xyzabc 123 qwerty asdf zxcv"

meaningful_features = analyzer.extract_full_features(meaningful_prompt)
meaningless_features = analyzer.extract_full_features(meaningless_prompt)

print(f"\n  æ„å‘³ã®ã‚ã‚‹æ–‡ vs ç„¡æ„å‘³ãªæ–‡:")
for i in range(model.config.n_layer):
    m_ttfs = meaningful_features.get(f'layer{i}_attn_ttfs', 0)
    n_ttfs = meaningless_features.get(f'layer{i}_attn_ttfs', 0)
    diff = n_ttfs - m_ttfs
    print(f"    Layer {i}: æ„å‘³={m_ttfs:.2f}, ç„¡æ„å‘³={n_ttfs:.2f} (å·®: {diff:+.2f})")


# =============================================================================
# 6. å¯è¦–åŒ–
# =============================================================================
print("\nã€5. å¯è¦–åŒ–ã€‘")

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# å±¤ã”ã¨ã®Attention TTFS
ax = axes[0, 0]
layers = list(range(model.config.n_layer))
meaningful_ttfs = [meaningful_features.get(f'layer{i}_attn_ttfs', 0) for i in layers]
meaningless_ttfs = [meaningless_features.get(f'layer{i}_attn_ttfs', 0) for i in layers]
ax.plot(layers, meaningful_ttfs, 'go-', label='Meaningful', linewidth=2, markersize=8)
ax.plot(layers, meaningless_ttfs, 'ro-', label='Meaningless', linewidth=2, markersize=8)
ax.set_xlabel('Layer')
ax.set_ylabel('Attention TTFS')
ax.set_title('Attention TTFS by Layer (GPT-2)')
ax.legend()
ax.grid(True, alpha=0.3)

# HeadåŒæœŸåº¦
ax = axes[0, 1]
meaningful_sync = [meaningful_features.get(f'layer{i}_head_sync', 0) for i in layers]
meaningless_sync = [meaningless_features.get(f'layer{i}_head_sync', 0) for i in layers]
ax.plot(layers, meaningful_sync, 'go-', label='Meaningful', linewidth=2, markersize=8)
ax.plot(layers, meaningless_sync, 'ro-', label='Meaningless', linewidth=2, markersize=8)
ax.set_xlabel('Layer')
ax.set_ylabel('Head Synchrony')
ax.set_title('Multi-Head Synchrony by Layer')
ax.legend()
ax.grid(True, alpha=0.3)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆ¥ã®å‡ºåŠ›ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
ax = axes[1, 0]
prompts_short = [p[:15] + '...' for p in test_prompts]
entropies = [f['output_entropy'] for f in all_features]
colors = ['green', 'green', 'orange', 'red']
ax.barh(prompts_short, entropies, color=colors)
ax.set_xlabel('Output Entropy')
ax.set_title('Output Entropy by Prompt Type')

# ç”Ÿæˆå®‰å®šæ€§
ax = axes[1, 1]
prompts_short = [r['prompt'][:15] + '...' for r in stability_results]
stabilities = [r['stability'] for r in stability_results]
ax.bar(prompts_short, stabilities, color=['green', 'green', 'orange'])
ax.set_ylabel('Stability Score')
ax.set_title('Generation Stability')
ax.set_ylim(0, 1.1)

plt.tight_layout()
plt.savefig('gpt2_snn_analysis.png', dpi=150, bbox_inches='tight')
print("  ä¿å­˜: gpt2_snn_analysis.png")


# =============================================================================
# 7. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥æŒ‡æ¨™ã®ææ¡ˆ
# =============================================================================
print("\n" + "=" * 70)
print("ğŸ”¬ GPT-2 SNN Analysis ã¾ã¨ã‚")
print("=" * 70)

print(f"""
ã€æ‰‹æ³•ã€‘
  - Attentioné‡ã¿ â†’ TTFSå¤‰æ› â†’ ãƒˆãƒ¼ã‚¯ãƒ³é‡è¦åº¦
  - Multi-HeadåŒæœŸåº¦ â†’ æ¦‚å¿µã®ä¸€è²«æ€§
  - ç”Ÿæˆå®‰å®šæ€§ â†’ åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®å‡ºåŠ›ã®æºã‚Œ

ã€ä¸»è¦ç™ºè¦‹ã€‘

  1. ç„¡æ„å‘³ãªå…¥åŠ›ã¯ Attention TTFS ãŒé«˜ã„å‚¾å‘
     - æ„å‘³: {np.mean(meaningful_ttfs):.2f}
     - ç„¡æ„å‘³: {np.mean(meaningless_ttfs):.2f}
     â†’ ç„¡æ„å‘³å…¥åŠ›ã¯AttentionãŒã€Œè¿·ã£ã¦ã„ã‚‹ã€
  
  2. HeadåŒæœŸåº¦ã¯å…¥åŠ›ã®æ˜ç¢ºã•ã‚’åæ˜ 
     - æ˜ç¢ºãªè³ªå• â†’ é«˜åŒæœŸ
     - æ›–æ˜§ãªå…¥åŠ› â†’ ä½åŒæœŸ
  
  3. å‡ºåŠ›ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™ã«
     - ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = ç¢ºä¿¡åº¦é«˜
     - é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = ä¸ç¢ºå®Ÿ

ã€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œçŸ¥ã¸ã®å¿œç”¨ã€‘

  ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ = 
    (å‡ºåŠ›ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ Ã— 0.4) + 
    (1 - HeadåŒæœŸåº¦ Ã— 0.3) +
    (1 - ç”Ÿæˆå®‰å®šæ€§ Ã— 0.3)

  é«˜ã‚¹ã‚³ã‚¢ = ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯é«˜

ã€LLMã¸ã®å±•æœ›ã€‘
  - GPT-3/4, Llama, Claudeç­‰ã«ã‚‚åŒæ§˜ã®æ‰‹æ³•ãŒé©ç”¨å¯èƒ½
  - Attentionå±¤ã®TTFSè§£æã¯æ±ç”¨çš„
  - æ¨è«–æ™‚ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã«æ´»ç”¨å¯èƒ½
""")

print("\nğŸš€ GPT-2ã®Attention = ã‚¹ãƒ‘ã‚¤ã‚¯ã§å¯è¦–åŒ–å¯èƒ½ï¼")
print("=" * 70)
