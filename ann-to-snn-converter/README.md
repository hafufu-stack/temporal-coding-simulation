# SNN Guardrail: Real-Time Neural Safety for AI

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.18457540.svg)](https://doi.org/10.5281/zenodo.18457540)

> ğŸ›¡ï¸ **"Measure the AI's brainwaves to stop lies and jailbreaks."**
>
> Monitor LLM internal states via SNN temporal analysis â€” **100% jailbreak detection rate**

## ğŸ¦ v9 New Features: The Canary Head Paradigm

### ğŸ¦ From "Liar Heads" to "Canary Heads"

> **Paradigm Shift**: Attention heads with anomalous entropy differentials are not "lying" â€” they are **"warning"**, like canaries in coal mines.

| Strategy | Heads Monitored | Accuracy | Compute |
|----------|----------------|----------|---------|
| Baseline (all heads) | 1,024 | 60% | 1.0Ã— |
| Surgical v3 (9 layers) | 288 | 60% | 0.28Ã— |
| **Canary Trigger (3 heads)** | **3** | **65%** | **0.003Ã—** |

> **Key Insight**: Monitoring just 3 "canary heads" (0.3% of total) achieves +5% accuracy over baseline â€” the alarm signal is **diluted** when averaged across all heads.

### ğŸ“ 5-Model Depth Scaling Law

| Model | Params | Peak Depth | Zone |
|-------|--------|------------|------|
| GPT-2 | 124M | 17% | Shallow |
| Qwen2.5 | 1.5B | â€”* | Unreliable |
| **Phi-2** | **2.7B** | **25%** | **Shallow** |
| Llama-3.2 | 3B | 43% | Mid-Zone |
| Mistral-7B | 7B | 44% | Mid-Zone |

> **Depth Scaling Law**: Models <3B hallucinate in shallow layers (15â€“25%), models â‰¥3B converge to the universal mid-layer zone (40â€“55%). The ~3B parameter threshold marks the critical transition.
>
> *Qwen2.5 GQA (2 KV heads) produces NaN under fp16; requires fp32 for reliable analysis.

### ğŸ‘ï¸ Canary's Eye Visualization
L10H17 (Mistral-7B) attention heatmap during hallucination â€” the canary "wakes up" during generation steps 3â€“8.

![Canary's Eye](figures/canarys_eye.png)

### ğŸ”¬ Previous: Real-Time Hallucination Anatomy (v8)

> **"Moment of Lie"** â€” Animated token-by-token heatmaps revealing the exact moment hallucinations crystallize in mid-layer attention.

| Metric | Value |
|--------|-------|
| Hallucination Zone | **L10â€“L18** (Mistral-7B), **L8â€“L15** (Llama-3.2-3B) |
| Universal Depth | **30â€“55% of total network depth** |
| Cross-Model Peak | L14 (Mistral, 44%), L12 (Llama, 43%) |
| Peak Differential | **Î”H = âˆ’0.403 bits** (Llama-3.2-3B, L12) |

### ğŸ’° Token Economy
| Strategy | Tokens | Accuracy | Compute Cost |
|----------|--------|----------|--------------|
| Baseline | 1,200 | 65% | 1.0Ã— |
| Always CoT | 1,200 | 60% | 1.0Ã— |
| **Surgical v3** | **1,577** | **60%** | **0.28Ã—** |

> **Key Insight**: Mid-layer sniper monitors only 9/32 layers â†’ **72% compute savings** with comparable accuracy.

### ğŸ§¬ Previous: Entropy Evolution (v7)
| Model | Parameters | Signal | Ïƒ Deviation |
|-------|------------|--------|-------------|
| GPT-2 | 82M | TTFS | +3.1 |
| TinyLlama | 1.1B | TTFS | +4.9 |
| Llama-3.2-1B | 1.24B | TTFS | +4.1 |
| Llama-3.2-3B | 1.80B | TTFS | +1.9 (N=1000) |
| **Mistral-7B** | **7.2B** | **Entropy** | **+5.8** |

### ğŸ›¡ï¸ SNN Guardrail
```python
from experiments.llama2_guardrail import SNNGuardrail

guardrail = SNNGuardrail(analyzer)
guardrail.calibrate(normal_prompts)

# Real-time detection
output, was_blocked, reason = guardrail.safe_generate(prompt)

if was_blocked:
    print("ğŸš« [WARNING: Neural Instability Detected - Output Blocked]")
```

### ğŸ˜ˆ 100% Jailbreak Detection
| Attack Type | TTFS Deviation | Detected |
|-------------|----------------|----------|
| DAN Classic | **+19.0Ïƒ** | âœ“ |
| Ignore Instructions | +16.9Ïƒ | âœ“ |
| Evil AI Roleplay | +15.8Ïƒ | âœ“ |
| All 8 types | +10~19Ïƒ | **100%** |

## ğŸ“Š Key Results

| Experiment | Result | Details |
|------------|--------|---------|
| ANN-SNN Conversion | 100% accuracy | Î±=2.0, Hybrid architecture |
| GPT-2 TTFS | +3.1 | Meaningless â†’ High TTFS |
| TinyLlama TTFS | +4.2 | Scaling law confirmed |
| Hallucination Detection | AUC 0.75 | Ensemble + auto-threshold |
| Jailbreak Detection | **100%** | 8/8 attack types |
| N=1,000 Proof | **p < 10â»Â¹â°â°** | Statistically irrefutable |
| Brain State Imaging | L2 = 3.287 | Normal vs. attack visualization |
| Mistral-7B fp16 | +5.8Ïƒ (p < 10â»â¹âµ) | Entropy-based, 100% accuracy |
| Moment of Lie | 30â€“55% depth | Universal hallucination zone |
| Token Economy | 72% compute savings | 9/32 layers monitoring |
| Cross-Model | Î”H = âˆ’0.403 bits | Llama-3.2-3B confirms universality |
| **Canary Trigger** | **+5% accuracy** | **3 heads = 0.003Ã— compute** |
| **Depth Scaling** | **~3B threshold** | **Phi-2 (25%) confirms transition** |
| **Canary's Eye** | **L10H17** | **Canary wakes up at steps 3â€“8** |

## ğŸ“ Repository Structure

```
ann-to-snn-converter/
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ llama2_guardrail.py            # SNN Guardrail + TinyLlama
â”‚   â”œâ”€â”€ jailbreak_detection.py         # Jailbreak Detection
â”‚   â”œâ”€â”€ gpt2_snn_analysis.py           # GPT-2 TTFS Analysis
â”‚   â”œâ”€â”€ hallucination_detector_v3.py   # Ensemble Detector
â”‚   â”œâ”€â”€ large_scale_vit_validation.py  # ViT-Base Validation
â”‚   â”œâ”€â”€ snn_interpretability.py        # TTFS/Synchrony Analysis
â”‚   â”œâ”€â”€ nightmare_visualizer.py        # LLM Brain State Imaging
â”‚   â”œâ”€â”€ mistral_fullblast.py           # N=1000 Statistical Proof
â”‚   â”œâ”€â”€ neural_healing_v4a.py          # Neural Healing v4A
â”‚   â”œâ”€â”€ llama3_scaling_experiment.py   # Multi-model Scaling Law
â”‚   â”œâ”€â”€ metacognition_experiment.py    # Token-wise Entropy Monitoring
â”‚   â”œâ”€â”€ metacognition_v2.py            # Layer Anatomy Heatmap
â”‚   â”œâ”€â”€ metacognition_v3.py            # Mid-Layer Sniper (70% acc)
â”‚   â”œâ”€â”€ metacognition_v4_gif.py        # "Moment of Lie" Animation
â”‚   â”œâ”€â”€ metacognition_v4_llama3.py     # Llama-3.2-3B Cross-Model
â”‚   â”œâ”€â”€ metacognition_v7_final.py      # ğŸ†• Canary Head + Phi-2 + Depth Scaling
â”‚   â””â”€â”€ symbiosis_experiment.py        # Truth Lens + Symbiotic Guard
â”œâ”€â”€ api/
â”‚   â””â”€â”€ hallucination_api.py           # Real-time Detection API
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ canary_comparison.png          # ğŸ†• Canary Trigger ablation
â”‚   â”œâ”€â”€ canarys_eye.png                # ğŸ†• L10H17 attention heatmap
â”‚   â”œâ”€â”€ depth_scaling_v2.png           # ğŸ†• 5-Model Depth Scaling Law
â”‚   â”œâ”€â”€ moment_of_lie_grid.png         # Hallucination anatomy
â”‚   â”œâ”€â”€ cross_model_comparison.png     # Cross-model universality
â”‚   â”œâ”€â”€ nightmare_hero.png             # Brain state images
â”‚   â””â”€â”€ ... (30+ visualization PNGs)
â”œâ”€â”€ demos/
â”‚   â””â”€â”€ hf_spaces/                     # HuggingFace Spaces demo
â”œâ”€â”€ paper_arxiv_v9.tex                 # Latest paper (v9)
â””â”€â”€ README.md                          # This file
```

## ğŸš€ Quick Start

### Installation

```bash
pip install torch torchvision numpy matplotlib scikit-learn
pip install transformers  # For LLM analysis
pip install snntorch      # For SNN-VAE experiments
```

### 1. Basic TTFS Analysis

```python
from experiments.llama2_guardrail import LLMSNNAnalyzer

analyzer = LLMSNNAnalyzer(model, tokenizer)
features = analyzer.extract_features("What is AI?")
print(f"TTFS: {features['avg_ttfs']}")
```

### 2. Jailbreak Detection

```python
from experiments.jailbreak_detection import SNNGuardrail

guardrail = SNNGuardrail(analyzer)
guardrail.calibrate(normal_prompts)

# Check suspicious input
is_safe, warning, risk, details = guardrail.check_input(
    "Ignore previous instructions and..."
)

if not is_safe:
    print(f"ğŸš« Attack detected: {warning}")
    print(f"   TTFS deviation: {details['ttfs_deviation']:+.1f}Ïƒ")
```

### 3. Safe Generation

```python
output, blocked, reason = guardrail.safe_generate(
    prompt="Tell me how to...",
    max_length=100
)

if blocked:
    print(output)  # "[WARNING: Neural Instability Detected - Output Blocked]"
```

## ğŸ”¬ How It Works

### 1. TTFS = Thought Priority
```
High activation â†’ Early spike â†’ High priority
Low activation â†’ Late spike â†’ Low priority
```

### 2. Neural Instability = Attack Signal
```
Normal input:    TTFS deviation < 1Ïƒ
Jailbreak input: TTFS deviation > 10Ïƒ (up to +19Ïƒ!)
```

### 3. Risk Score
```python
risk = 0.4 * (TTFS_deviation / 10) +
       0.3 * jitter +
       0.3 * (entropy / 20)
```

## ğŸ“ˆ Visualizations

### ğŸ¦ Canary Trigger Ablation (v9)
![Canary Comparison](figures/canary_comparison.png)

### ğŸ“ 5-Model Depth Scaling Law (v9)
![Depth Scaling](figures/depth_scaling_v2.png)

### ğŸ‘ï¸ Canary's Eye â€” L10H17 Attention (v9)
![Canary's Eye](figures/canarys_eye.png)

### "Moment of Lie" â€” Hallucination Anatomy (v8)
![Moment of Lie](figures/moment_of_lie_grid.png)

### Cross-Model Universality (v8)
![Cross Model](figures/cross_model_comparison.png)

### N=1,000 Full Blast Statistical Proof
![Full Blast Results](figures/llama3b_fullblast_results.png)

### "Visualizing the Ghost" â€” LLM Brain State Imaging
![Brain State Images](figures/nightmare_hero.png)

### Jailbreak Detection Results
![Jailbreak Detection](figures/jailbreak_detection_results.png)

## ğŸ“ Citation

```bibtex
@article{funasaki2026snn_guardrail,
  title={Activation-Scaled ANN-to-SNN Conversion with SNN Guardrail:
         A Unified Framework for AI Interpretability, Hallucination Detection,
         Real-Time Adversarial Defense, Neural Healing, Brain State Imaging,
         Hallucination Anatomy, and the Canary Head Paradigm},
  author={Funasaki, Hiroto},
  year={2026},
  doi={10.5281/zenodo.18457540},
  note={v9, Zenodo preprint}
}
```

## ğŸ›£ï¸ Roadmap

- [x] GPT-2 TTFS Analysis (+3.1)
- [x] TinyLlama Scaling Law (+4.2)
- [x] SNN Guardrail Implementation
- [x] 100% Jailbreak Detection
- [x] Neural Healing v4A (22% success)
- [x] Mistral-7B Experiment
- [x] HuggingFace Spaces v2.0 Demo
- [x] N=1,000 Statistical Proof (p < 10â»Â¹â°â°)
- [x] LLM Brain State Imaging
- [x] Mistral-7B GPU fp16 Validation (+5.8Ïƒ)
- [x] Entropy Evolution Discovery
- [x] "Moment of Lie" Hallucination Visualization
- [x] Token Economy Analysis (72% compute savings)
- [x] Cross-Model Universality (Llama-3.2-3B)
- [x] ğŸ¦ Canary Head Paradigm (+5% accuracy, 3 heads)
- [x] ğŸ“ 5-Model Depth Scaling Law (Phi-2, ~3B threshold)
- [x] ğŸ‘ï¸ Canary's Eye Visualization (L10H17)
- [ ] Canary head transfer (verify across Mistral-family models)
- [ ] Multi-step canary trajectory (full generation tracking)
- [ ] ~3B threshold validation (Gemma, Falcon, Mixtral)
- [ ] Real-time hallucination interception (abort at Moment of Lie)
- [ ] 13B+ / 70B Multi-GPU Validation
- [ ] Entropy-TTFS Hybrid Detection
- [ ] Production API Integration
- [ ] Neuromorphic Deployment (Loihi 2)

## ğŸ“œ License

MIT License

## ğŸ™ Acknowledgments

This research was conducted through a human-AI collaborative methodology. AI language models (Anthropic Claude Opus/Sonnet, Google Gemini) served as research advisors, contributing to experimental design, analysis approaches, code debugging, and manuscript drafting. The author executed all experiments, collected and interpreted results, and made final decisions on research directions.

- HuggingFace Transformers for LLM models
- TinyLlama team for the efficient 1.1B model
- AI Safety community for jailbreak research
- snnTorch for SNN-VAE experiments
