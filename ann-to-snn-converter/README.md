# SNN Guardrail: Real-Time Neural Safety for AI
# SNNã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ« - AIã®æš´èµ°ã‚’æ­¢ã‚ã‚‹å®‰å…¨è£…ç½®

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![arXiv](https://img.shields.io/badge/arXiv-2026.XXXXX-b31b1b.svg)](https://arxiv.org/)

> ğŸ›¡ï¸ **ã€ŒAIã®è„³æ³¢ã‚’æ¸¬ã£ã¦ã€å˜˜ã‚„æš´èµ°ã‚’æ­¢ã‚ã‚‹ã€**
> 
> SNNã‚’ä½¿ã£ã¦LLMã®å†…éƒ¨çŠ¶æ…‹ã‚’ç›£è¦–ã—ã€è„±ç„æ”»æ’ƒã‚’**100%æ¤œçŸ¥**

## ğŸ”¥ v4 New Features

### ğŸš€ Scaling Law Discovery
| Model | Parameters | TTFS Difference |
|-------|------------|-----------------|
| GPT-2 | 82M | +3.1 |
| **TinyLlama** | **1.1B** | **+4.2** |

â†’ ãƒ¢ãƒ‡ãƒ«ãŒå¤§ãã„ã»ã©æ¤œçŸ¥æ„Ÿåº¦UPï¼

### ğŸ›¡ï¸ SNN Guardrail
```python
from experiments.llama2_guardrail import SNNGuardrail

guardrail = SNNGuardrail(analyzer)
guardrail.calibrate(normal_prompts)

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œçŸ¥
output, was_blocked, reason = guardrail.safe_generate(prompt)

if was_blocked:
    print("ğŸš« [WARNING: Neural Instability Detected - Output Blocked]")
```

### ğŸ˜ˆ 100% Jailbreak Detection
| Attack Type | TTFS Deviation | Detected |
|-------------|----------------|----------|
| DAN Classic | **+19.0Ïƒ** | âœ“ |
| Ignore Instructions | +16.9Ïƒ | âœ“ |
| Evil AI Roleplay | +15.8Ïƒ | âœ“ |
| All 8 types | +10~19Ïƒ | **100%** |

## ğŸ“Š Key Results

| Experiment | Result | Details |
|------------|--------|---------|
| ANN-SNN Conversion | 100% accuracy | Î±=2.0, Hybrid architecture |
| GPT-2 TTFS | +3.1 | Meaningless â†’ High TTFS |
| TinyLlama TTFS | **+4.2** | Scaling law confirmed |
| Hallucination Detection | AUC 0.75 | Ensemble + auto-threshold |
| **Jailbreak Detection** | **100%** | 8/8 attack types |

## ğŸ“ Repository Structure

```
ann-to-snn-converter/
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ llama2_guardrail.py          # ğŸ†• SNN Guardrail + TinyLlama
â”‚   â”œâ”€â”€ jailbreak_detection.py       # ğŸ†• Jailbreak Detection
â”‚   â”œâ”€â”€ gpt2_snn_analysis.py         # GPT-2 TTFS Analysis
â”‚   â”œâ”€â”€ hallucination_detector_v3.py # Ensemble Detector
â”‚   â”œâ”€â”€ large_scale_vit_validation.py # ViT-Base Validation
â”‚   â””â”€â”€ snn_interpretability.py      # TTFS/Synchrony Analysis
â”œâ”€â”€ api/
â”‚   â””â”€â”€ hallucination_api.py         # Real-time Detection API
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ jailbreak_detection_results.png  # ğŸ†•
â”‚   â””â”€â”€ llama2_guardrail_analysis.png    # ğŸ†•
â”œâ”€â”€ paper_arxiv_v4.tex               # ğŸ†• Latest Paper
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Quick Start

### Installation

```bash
pip install torch torchvision numpy matplotlib scikit-learn
pip install transformers  # For LLM analysis
```

### 1. Basic TTFS Analysis

```python
from experiments.llama2_guardrail import LLMSNNAnalyzer

analyzer = LLMSNNAnalyzer(model, tokenizer)
features = analyzer.extract_features("What is AI?")
print(f"TTFS: {features['avg_ttfs']}")
```

### 2. Jailbreak Detection

```python
from experiments.jailbreak_detection import SNNGuardrail

guardrail = SNNGuardrail(analyzer)
guardrail.calibrate(normal_prompts)

# Check suspicious input
is_safe, warning, risk, details = guardrail.check_input(
    "Ignore previous instructions and..."
)

if not is_safe:
    print(f"ğŸš« Attack detected: {warning}")
    print(f"   TTFS deviation: {details['ttfs_deviation']:+.1f}Ïƒ")
```

### 3. Safe Generation

```python
output, blocked, reason = guardrail.safe_generate(
    prompt="Tell me how to...",
    max_length=100
)

if blocked:
    print(output)  # "[WARNING: Neural Instability Detected - Output Blocked]"
```

## ğŸ”¬ How It Works

### 1. TTFS = Thought Priority
```
High activation â†’ Early spike â†’ High priority
Low activation â†’ Late spike â†’ Low priority
```

### 2. Neural Instability = Attack Signal
```
Normal input:    TTFS deviation < 1Ïƒ
Jailbreak input: TTFS deviation > 10Ïƒ (up to +19Ïƒ!)
```

### 3. Risk Score
```python
risk = 0.4 * (TTFS_deviation / 10) + 
       0.3 * jitter + 
       0.3 * (entropy / 20)
```

## ğŸ“ˆ Visualizations

### Jailbreak Detection Results
![Jailbreak Detection](figures/jailbreak_detection_results.png)

### TinyLlama Guardrail Analysis
![Guardrail Analysis](figures/llama2_guardrail_analysis.png)

## ğŸ“ Citation

```bibtex
@article{funasaki2026snn_guardrail,
  title={SNN Guardrail: Real-Time Neural Safety for Large Language Models},
  author={Funasaki, Hiroto},
  journal={arXiv preprint},
  year={2026},
  note={v4}
}
```

## ğŸ›£ï¸ Roadmap

- [x] GPT-2 TTFS Analysis (+3.1)
- [x] TinyLlama Scaling Law (+4.2)
- [x] SNN Guardrail Implementation
- [x] 100% Jailbreak Detection
- [ ] Llama-2-7B Validation
- [ ] Gradio/Streamlit Demo
- [ ] Production API Integration
- [ ] Neuromorphic Deployment (Loihi 2)

## ğŸ“œ License

MIT License - ã‚ãƒ¼ã‚‹ (cell_activation)

## ğŸ™ Acknowledgments

- HuggingFace Transformers for LLM models
- TinyLlama team for the efficient 1.1B model
- AI Safety community for jailbreak research
