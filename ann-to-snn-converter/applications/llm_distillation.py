"""
LLMè’¸ç•™ with 10é€²æ•°ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
==============================

å¤§è¦æ¨¡LLMï¼ˆRinna, Qwenç­‰ï¼‰ã®çŸ¥è­˜ã‚’
10é€²æ•°ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«è’¸ç•™ã™ã‚‹

ç›®æ¨™:
- 7B â†’ 7Mï¼ˆ1000å€å°å‹åŒ–ï¼‰
- æ—¥æœ¬èªèƒ½åŠ›ã‚’ç¶­æŒ
- ãƒ­ãƒ¼ã‚«ãƒ«ã§é«˜é€Ÿå‹•ä½œ

Author: ã‚ãƒ¼ã‚‹ (cell_activation)
Date: 2026-01-31
"""

import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import json
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.evolved_decimal_neuron import (
    EvolvedDecimalNeuron, 
    AdamOptimizer,
    DecimalLanguageModel
)


# =============================================================================
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# =============================================================================

@dataclass
class Token:
    """ãƒˆãƒ¼ã‚¯ãƒ³"""
    id: int
    text: str
    probability: float = 1.0


@dataclass
class TrainingExample:
    """å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«"""
    input_text: str
    target_text: str
    teacher_logits: Optional[np.ndarray] = None


# =============================================================================
# 10é€²æ•°ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
# =============================================================================

class DecimalTokenizer:
    """
    10é€²æ•°ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    
    æ–‡å­—ã‚’10é€²æ•°ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›
    UTF-8ã‚³ãƒ¼ãƒ‰ã‚’10é€²æ•°ã§è¡¨ç¾
    """
    
    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = vocab_size
        self.char_to_id: Dict[str, int] = {}
        self.id_to_char: Dict[int, str] = {}
        
        # åŸºæœ¬çš„ãªæ–‡å­—ï¼ˆASCII + æ—¥æœ¬èªï¼‰
        self._build_vocab()
    
    def _build_vocab(self):
        """èªå½™ã‚’æ§‹ç¯‰"""
        # ASCII
        for i in range(128):
            char = chr(i) if 32 <= i < 127 else f"<{i}>"
            self.char_to_id[char] = i
            self.id_to_char[i] = char
        
        # æ—¥æœ¬èªã²ã‚‰ãŒãª
        for i, code in enumerate(range(0x3040, 0x30A0)):
            char = chr(code)
            idx = 128 + i
            self.char_to_id[char] = idx
            self.id_to_char[idx] = char
        
        # æ—¥æœ¬èªã‚«ã‚¿ã‚«ãƒŠ
        for i, code in enumerate(range(0x30A0, 0x3100)):
            char = chr(code)
            idx = 224 + i
            self.char_to_id[char] = idx
            self.id_to_char[idx] = char
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
        self.char_to_id["<PAD>"] = 0
        self.char_to_id["<UNK>"] = 1
        self.char_to_id["<BOS>"] = 2
        self.char_to_id["<EOS>"] = 3
        self.id_to_char[0] = "<PAD>"
        self.id_to_char[1] = "<UNK>"
        self.id_to_char[2] = "<BOS>"
        self.id_to_char[3] = "<EOS>"
    
    def encode(self, text: str) -> List[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›"""
        ids = [self.char_to_id.get("<BOS>", 2)]
        for char in text:
            if char in self.char_to_id:
                ids.append(self.char_to_id[char])
            else:
                # æœªçŸ¥æ–‡å­—ã¯Unicodeã‚³ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
                ids.append(ord(char) % self.vocab_size)
        ids.append(self.char_to_id.get("<EOS>", 3))
        return ids
    
    def decode(self, ids: List[int]) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        chars = []
        for id in ids:
            if id in [0, 2, 3]:  # PAD, BOS, EOS
                continue
            if id in self.id_to_char:
                chars.append(self.id_to_char[id])
            else:
                try:
                    chars.append(chr(id))
                except:
                    chars.append("?")
        return "".join(chars)
    
    def to_decimal_sequence(self, ids: List[int]) -> List[List[int]]:
        """ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’10é€²æ•°ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›"""
        # å„IDã‚’4æ¡ã®10é€²æ•°ã«å¤‰æ› (0-9999)
        decimal_seq = []
        for id in ids:
            digits = [(id // 1000) % 10, (id // 100) % 10, 
                     (id // 10) % 10, id % 10]
            decimal_seq.append(digits)
        return decimal_seq
    
    def from_decimal_sequence(self, decimal_seq: List[List[int]]) -> List[int]:
        """10é€²æ•°ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›"""
        ids = []
        for digits in decimal_seq:
            if len(digits) >= 4:
                id = digits[0] * 1000 + digits[1] * 100 + digits[2] * 10 + digits[3]
            else:
                id = sum(d * (10 ** (len(digits) - 1 - i)) for i, d in enumerate(digits))
            ids.append(id % self.vocab_size)
        return ids


# =============================================================================
# 10é€²æ•°LLM
# =============================================================================

class DecimalLLM:
    """
    10é€²æ•°ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ™ãƒ¼ã‚¹ã®LLM
    
    ç‰¹å¾´:
    - 10é€²æ•°å…¥å‡ºåŠ›
    - ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã§æ–‡è„ˆç†è§£
    - è’¸ç•™ã§å¤§å‹LLMã®çŸ¥è­˜ã‚’ç¶™æ‰¿
    """
    
    def __init__(self, hidden_size: int = 32, n_layers: int = 4, 
                 context_length: int = 64):
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.context_length = context_length
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        self.tokenizer = DecimalTokenizer()
        
        # åŸ‹ã‚è¾¼ã¿å±¤ï¼ˆ4æ¡ Ã— hidden_sizeï¼‰
        self.embed_neurons = [[EvolvedDecimalNeuron() for _ in range(4)]
                              for _ in range(hidden_size)]
        
        # éš ã‚Œå±¤ï¼ˆã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ï¼‰
        self.hidden_layers = []
        for layer in range(n_layers):
            neurons = [EvolvedDecimalNeuron() for _ in range(hidden_size)]
            # éš£æ¥ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«
            for i in range(hidden_size - 1):
                neurons[i].entangle(neurons[i + 1])
            self.hidden_layers.append(neurons)
        
        # å‡ºåŠ›å±¤ï¼ˆ4æ¡ï¼‰
        self.output_neurons = [[EvolvedDecimalNeuron() for _ in range(4)]
                               for _ in range(hidden_size)]
        
        # æ–‡è„ˆãƒ¡ãƒ¢ãƒª
        self.context_memory: List[np.ndarray] = []
        
        # å­¦ç¿’çµ±è¨ˆ
        self.training_loss = []
        self.accuracy_history = []
    
    def embed(self, token_id: int) -> np.ndarray:
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŸ‹ã‚è¾¼ã¿"""
        # 4æ¡ã«åˆ†è§£
        digits = [(token_id // 1000) % 10, (token_id // 100) % 10,
                 (token_id // 10) % 10, token_id % 10]
        
        # å„æ¡ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§å‡¦ç†
        embedding = np.zeros(self.hidden_size)
        for i in range(min(self.hidden_size, len(self.embed_neurons))):
            for j, d in enumerate(digits):
                state = self.embed_neurons[i][j].forward(d)
                embedding[i] += self.embed_neurons[i][j].decode(state) / 4
        
        return embedding
    
    def forward_hidden(self, x: np.ndarray) -> np.ndarray:
        """éš ã‚Œå±¤ã‚’é€šã™"""
        current = x
        
        for layer_idx, layer in enumerate(self.hidden_layers):
            next_state = np.zeros(self.hidden_size)
            
            for i, neuron in enumerate(layer):
                # å…¥åŠ›ã‚’10é€²æ•°ã«å¤‰æ›
                input_digit = int(current[i] * 9) % 10
                state = neuron.forward(input_digit)
                next_state[i] = neuron.decode(state) / 9
            
            current = next_state
        
        return current
    
    def output_token(self, hidden: np.ndarray) -> int:
        """éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ"""
        digits = [0, 0, 0, 0]
        
        # å„ä½ã‚’è¨ˆç®—
        for digit_pos in range(4):
            votes = np.zeros(10)
            
            for i in range(min(self.hidden_size, len(self.output_neurons))):
                input_digit = int(hidden[i] * 9) % 10
                state = self.output_neurons[i][digit_pos].forward(input_digit)
                predicted = self.output_neurons[i][digit_pos].decode(state)
                votes[predicted] += 1
            
            digits[digit_pos] = int(np.argmax(votes))
        
        token_id = digits[0] * 1000 + digits[1] * 100 + digits[2] * 10 + digits[3]
        return token_id % self.tokenizer.vocab_size
    
    def forward(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†"""
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        token_ids = self.tokenizer.encode(text)
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†
        output_ids = []
        for token_id in token_ids[:-1]:  # EOSã‚’é™¤ã
            # åŸ‹ã‚è¾¼ã¿
            embedding = self.embed(token_id)
            
            # æ–‡è„ˆã‚’è¿½åŠ 
            if self.context_memory:
                context = np.mean(self.context_memory[-self.context_length:], axis=0)
                embedding = 0.7 * embedding + 0.3 * context
            
            # éš ã‚Œå±¤
            hidden = self.forward_hidden(embedding)
            
            # æ–‡è„ˆãƒ¡ãƒ¢ãƒªæ›´æ–°
            self.context_memory.append(hidden)
            if len(self.context_memory) > self.context_length:
                self.context_memory.pop(0)
            
            # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³
            output_id = self.output_token(hidden)
            output_ids.append(output_id)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        return self.tokenizer.decode(output_ids)
    
    def generate(self, prompt: str, max_length: int = 20) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        token_ids = self.tokenizer.encode(prompt)
        generated = list(token_ids)
        
        for _ in range(max_length):
            # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰æ¬¡ã‚’äºˆæ¸¬
            embedding = self.embed(generated[-1])
            
            if self.context_memory:
                context = np.mean(self.context_memory[-self.context_length:], axis=0)
                embedding = 0.7 * embedding + 0.3 * context
            
            hidden = self.forward_hidden(embedding)
            self.context_memory.append(hidden)
            
            next_token = self.output_token(hidden)
            
            if next_token == self.tokenizer.char_to_id.get("<EOS>", 3):
                break
            
            generated.append(next_token)
        
        return self.tokenizer.decode(generated)
    
    def train_step(self, input_text: str, target_text: str):
        """1ã‚¹ãƒ†ãƒƒãƒ—å­¦ç¿’"""
        input_ids = self.tokenizer.encode(input_text)
        target_ids = self.tokenizer.encode(target_text)
        
        loss = 0
        correct = 0
        
        for i, (inp_id, tgt_id) in enumerate(zip(input_ids[:-1], target_ids[1:])):
            # é †ä¼æ’­
            embedding = self.embed(inp_id)
            hidden = self.forward_hidden(embedding)
            output_id = self.output_token(hidden)
            
            # æå¤±è¨ˆç®—
            if output_id == tgt_id:
                correct += 1
            else:
                loss += 1
            
            # é€†ä¼æ’­ï¼ˆå‡ºåŠ›å±¤ï¼‰
            target_digits = [(tgt_id // 1000) % 10, (tgt_id // 100) % 10,
                           (tgt_id // 10) % 10, tgt_id % 10]
            
            for j in range(min(self.hidden_size, len(self.output_neurons))):
                for k, target_d in enumerate(target_digits):
                    self.output_neurons[j][k].backward(target_d)
        
        accuracy = correct / max(1, len(input_ids) - 1)
        self.accuracy_history.append(accuracy)
        
        return loss, accuracy
    
    def distill_from_examples(self, examples: List[Tuple[str, str]], 
                              epochs: int = 10):
        """ä¾‹ã‹ã‚‰è’¸ç•™"""
        print(f"  {len(examples)}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã§è’¸ç•™ä¸­...")
        
        for epoch in range(epochs):
            total_loss = 0
            total_acc = 0
            
            for input_text, target_text in examples:
                loss, acc = self.train_step(input_text, target_text)
                total_loss += loss
                total_acc += acc
            
            avg_acc = total_acc / len(examples)
            
            if epoch % 5 == 0:
                print(f"    Epoch {epoch}: accuracy = {avg_acc:.2%}")
        
        return avg_acc
    
    def clear_context(self):
        """æ–‡è„ˆã‚’ã‚¯ãƒªã‚¢"""
        self.context_memory = []
    
    def get_stats(self) -> Dict:
        """çµ±è¨ˆã‚’å–å¾—"""
        return {
            "hidden_size": self.hidden_size,
            "n_layers": self.n_layers,
            "total_neurons": (
                self.hidden_size * 4 * 2 +  # embed + output
                self.hidden_size * self.n_layers  # hidden
            ),
            "context_length": self.context_length,
            "accuracy_history": self.accuracy_history[-10:] if self.accuracy_history else []
        }


# =============================================================================
# LLMè’¸ç•™ã‚·ã‚¹ãƒ†ãƒ 
# =============================================================================

class LLMDistiller:
    """
    å¤§å‹LLMã‹ã‚‰10é€²æ•°LLMã¸ã®è’¸ç•™
    
    ã‚¹ãƒ†ãƒƒãƒ—:
    1. æ•™å¸«LLMã‹ã‚‰å¿œç­”ã‚’åé›†
    2. å…¥åŠ›-å‡ºåŠ›ãƒšã‚¢ã‚’ä½œæˆ
    3. 10é€²æ•°LLMã‚’å­¦ç¿’
    """
    
    def __init__(self, student: DecimalLLM):
        self.student = student
        self.training_data: List[Tuple[str, str]] = []
    
    def add_training_pair(self, input_text: str, output_text: str):
        """å­¦ç¿’ãƒšã‚¢ã‚’è¿½åŠ """
        self.training_data.append((input_text, output_text))
    
    def create_japanese_training_data(self):
        """æ—¥æœ¬èªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        # åŸºæœ¬çš„ãªæ—¥æœ¬èªãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            # æŒ¨æ‹¶
            ("ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã¯ï¼"),
            ("ãŠã¯ã‚ˆã†", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™"),
            ("ã‚ã‚ŠãŒã¨ã†", "ã©ã†ã„ãŸã—ã¾ã—ã¦"),
            ("ã•ã‚ˆã†ãªã‚‰", "ã¾ãŸã­"),
            
            # è³ªå•å¿œç­”
            ("å¤©æ°—ã¯ï¼Ÿ", "ä»Šæ—¥ã¯æ™´ã‚Œã§ã™"),
            ("ä»Šä½•æ™‚ï¼Ÿ", "3æ™‚ã§ã™"),
            ("åå‰ã¯ï¼Ÿ", "ç§ã¯AIã§ã™"),
            
            # ç°¡å˜ãªä¼šè©±
            ("å…ƒæ°—ï¼Ÿ", "å…ƒæ°—ã§ã™ï¼"),
            ("ä½•ã—ã¦ã‚‹ï¼Ÿ", "å‹‰å¼·ä¸­ã§ã™"),
            ("å¥½ããªé£Ÿã¹ç‰©ã¯ï¼Ÿ", "ãƒ©ãƒ¼ãƒ¡ãƒ³ã§ã™"),
            
            # è¨ˆç®—
            ("1+1ã¯ï¼Ÿ", "2ã§ã™"),
            ("2Ã—3ã¯ï¼Ÿ", "6ã§ã™"),
            ("10Ã·2ã¯ï¼Ÿ", "5ã§ã™"),
            
            # ç¿»è¨³é¢¨
            ("Hello", "ã“ã‚“ã«ã¡ã¯"),
            ("Thank you", "ã‚ã‚ŠãŒã¨ã†"),
            ("Good morning", "ãŠã¯ã‚ˆã†"),
        ]
        
        for inp, out in patterns:
            self.add_training_pair(inp, out)
        
        print(f"  {len(patterns)}å€‹ã®æ—¥æœ¬èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ")
        return patterns
    
    def distill(self, epochs: int = 20):
        """è’¸ç•™ã‚’å®Ÿè¡Œ"""
        print("\n" + "=" * 50)
        print("ğŸ”¬ LLMè’¸ç•™é–‹å§‹")
        print("=" * 50)
        
        if not self.training_data:
            self.create_japanese_training_data()
        
        # è’¸ç•™
        final_acc = self.student.distill_from_examples(self.training_data, epochs)
        
        print(f"\n  æœ€çµ‚ç²¾åº¦: {final_acc:.2%}")
        return final_acc
    
    def evaluate(self) -> Dict:
        """è©•ä¾¡"""
        results = {
            "correct": 0,
            "total": 0,
            "examples": []
        }
        
        for input_text, expected in self.training_data[:5]:
            self.student.clear_context()
            output = self.student.generate(input_text, max_length=len(expected) + 5)
            
            is_correct = expected in output or output.startswith(expected[:3])
            results["total"] += 1
            if is_correct:
                results["correct"] += 1
            
            results["examples"].append({
                "input": input_text,
                "expected": expected,
                "output": output,
                "correct": is_correct
            })
        
        results["accuracy"] = results["correct"] / max(1, results["total"])
        return results


# =============================================================================
# ãƒ†ã‚¹ãƒˆ
# =============================================================================

def test_decimal_llm():
    """10é€²æ•°LLMãƒ†ã‚¹ãƒˆ"""
    
    print("\n" + "=" * 70)
    print("ğŸ§ª 10é€²æ•°LLM ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("\nã€ãƒ¢ãƒ‡ãƒ«ä½œæˆã€‘")
    llm = DecimalLLM(hidden_size=16, n_layers=2, context_length=32)
    stats = llm.get_stats()
    print(f"  ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°: {stats['total_neurons']}")
    print(f"  éš ã‚Œã‚µã‚¤ã‚º: {stats['hidden_size']}")
    print(f"  å±¤æ•°: {stats['n_layers']}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ
    print("\nã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€‘")
    test_texts = ["Hello", "ã“ã‚“ã«ã¡ã¯", "AI"]
    for text in test_texts:
        ids = llm.tokenizer.encode(text)
        decoded = llm.tokenizer.decode(ids)
        print(f"  '{text}' â†’ {ids[:5]}... â†’ '{decoded}'")
    
    # è’¸ç•™
    print("\nã€è’¸ç•™ã€‘")
    distiller = LLMDistiller(llm)
    distiller.distill(epochs=15)
    
    # è©•ä¾¡
    print("\nã€è©•ä¾¡ã€‘")
    results = distiller.evaluate()
    print(f"  ç²¾åº¦: {results['accuracy']:.2%}")
    
    for ex in results["examples"][:3]:
        status = "âœ“" if ex["correct"] else "âœ—"
        print(f"  {status} '{ex['input']}' â†’ '{ex['output'][:20]}...' (æœŸå¾…: '{ex['expected']}')")
    
    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print("\nã€ç”Ÿæˆãƒ†ã‚¹ãƒˆã€‘")
    llm.clear_context()
    prompts = ["ã“ã‚“ã«ã¡ã¯", "ã‚ã‚ŠãŒã¨ã†", "1+1ã¯"]
    for prompt in prompts:
        llm.clear_context()
        output = llm.generate(prompt, max_length=10)
        print(f"  '{prompt}' â†’ '{output}'")
    
    print("\n" + "=" * 70)
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
    print("=" * 70)
    
    # æ¯”è¼ƒè¡¨
    print("\n" + "=" * 70)
    print("ğŸ“Š ã‚µã‚¤ã‚ºæ¯”è¼ƒ")
    print("=" * 70)
    print("""
| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ã‚µã‚¤ã‚º | å‚™è€ƒ |
|--------|-----------|--------|------|
| Rinna 3.6B | 3,600,000,000 | ~7GB | å…ƒã®LLM |
| 10é€²æ•°LLM | ~2,000 | ~10KB | ç´„1,000,000å€å°ã•ã„ï¼ |

â€» æ€§èƒ½ã¯è½ã¡ã‚‹ãŒã€ç‰¹å®šã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã™ã‚Œã°ä½¿ãˆã‚‹ï¼
""")
    
    return llm, distiller


if __name__ == "__main__":
    llm, distiller = test_decimal_llm()
